{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "624fea1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(\"outputs/all_triples2.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    all_triples = json.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "be4d7f46",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\maadt\\AppData\\Local\\Temp\\ipykernel_4384\\773710034.py:41: DeprecationWarning: write_transaction has been renamed to execute_write\n",
      "  session.write_transaction(insert_triplet, src, rel, tgt)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 493 triplets insérés dans Neo4j.\n"
     ]
    }
   ],
   "source": [
    "from neo4j import GraphDatabase\n",
    "\n",
    "# ---------- 1. Configuration de la connexion Neo4j ----------\n",
    "# Remplacez ces valeurs par celles de votre instance Neo4j\n",
    "uri  = \"bolt://localhost:7687\"\n",
    "user = \"neo4j\"\n",
    "pwd  = \"BNEOsucks8921_\"\n",
    "\n",
    "# Crée un driver Neo4j\n",
    "driver = GraphDatabase.driver(uri, auth=(user, pwd))\n",
    "\n",
    "\n",
    "# ---------- 2. Fonction d’insertion d’un triplet dans Neo4j ----------\n",
    "def insert_triplet(tx, source, relation, target):\n",
    "    \"\"\"\n",
    "    Crée ou récupère deux nœuds (Entity) nommés 'source' et 'target', \n",
    "    puis crée (si nécessaire) la relation entre eux.\n",
    "    Le type de relation est stocké dans la propriété 'type' de l'arc.\n",
    "\n",
    "    Exemple Cypher généré :\n",
    "      MERGE (a:Entity {name: $source})\n",
    "      MERGE (b:Entity {name: $target})\n",
    "      MERGE (a)-[:RELATION {type: $relation}]->(b)\n",
    "    \"\"\"\n",
    "    query = \"\"\"\n",
    "    MERGE (a:Source {name: $source})\n",
    "    MERGE (b:Target {name: $target})\n",
    "    MERGE (a)-[:RELATION {type: $relation}]->(b)\n",
    "    \"\"\"\n",
    "    tx.run(query, source=source, relation=relation, target=target)\n",
    "\n",
    "\n",
    "# ---------- 3. Fonction de chargement de tous les triplets ----------\n",
    "def load_all_triplets(triplets):\n",
    "    \"\"\"\n",
    "    Parcourt la liste 'triplets' (liste de tuples (source, relation, target))\n",
    "    et exécute 'insert_triplet' pour chacun d’entre eux dans une même session.\n",
    "    \"\"\"\n",
    "    with driver.session() as session:\n",
    "        for src, rel, tgt in triplets:\n",
    "            session.write_transaction(insert_triplet, src, rel, tgt)\n",
    "    print(f\"✅ {len(triplets)} triplets insérés dans Neo4j.\")\n",
    "\n",
    "\n",
    "# ---------- 4. Point d’entrée du script ----------\n",
    "if __name__ == \"__main__\":\n",
    "    # Supposons que vous ayez, dans un autre module, votre liste 'all_triples'\n",
    "    # Exemple : all_triples = [(\"Navette autonome\", \"testée dans\", \"Toulouse\"), ...]\n",
    "    # Il faut donc importer ou recréer cette liste ici.\n",
    "    #\n",
    "    # Si votre extraction a produit un module Python ou un fichier pickle,\n",
    "    # récupérez la liste de tuples et assignez-la à 'all_triples'.\n",
    "    #\n",
    "    # Par exemple, si vous avez sauvegardé vos relations dans un fichier JSON :\n",
    "    #\n",
    "    # import json\n",
    "    # with open(\"outputs/all_triples.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    #     all_triples = json.load(f)\n",
    "    #\n",
    "    # Mais ici, illustrons un exemple statique (à remplacer par votre propre liste) :\n",
    "\n",
    "    # Charge tous les triplets dans Neo4j\n",
    "    load_all_triplets(all_triples)\n",
    "\n",
    "    # Ferme le driver une fois terminé\n",
    "   # driver.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "67e29929",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_all_triples():\n",
    "    \"\"\"\n",
    "    Récupère l'ensemble des triplets (entité, relation, entité) dans Neo4j.\n",
    "    \"\"\"\n",
    "    query = \"\"\"\n",
    "    MATCH (a:Source)-[r:RELATION]->(b:Target)\n",
    "    RETURN a.name AS source, r.type AS relation, b.name AS target\n",
    "    \"\"\"\n",
    "    triples = []\n",
    "    with driver.session() as session:\n",
    "        for record in session.run(query):\n",
    "            triples.append({\n",
    "                \"source\": record[\"source\"],\n",
    "                \"relation\": record[\"relation\"],\n",
    "                \"target\": record[\"target\"]\n",
    "            })\n",
    "    return triples\n",
    "\n",
    "# Exemple :\n",
    "all_triples = fetch_all_triples()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6888c69e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import llama_index\n",
    "# print(dir(llama_index))\n",
    "\n",
    "# import llama_index.core\n",
    "# print(dir(llama_index.core))\n",
    "\n",
    "# import llama_index.core.indices\n",
    "# print(dir(llama_index.core.indices))\n",
    "\n",
    "\n",
    "# import llama_index.indices\n",
    "# print(dir(llama_index.indices))\n",
    "\n",
    "# import llama_index.llms\n",
    "# print(dir(llama_index.llms))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "23761c21",
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import pkgutil\n",
    "\n",
    "def deep_dir(module):\n",
    "    results = {}\n",
    "\n",
    "    def explore(mod, prefix=\"\"):\n",
    "        name = prefix + (mod.__name__ if hasattr(mod, \"__name__\") else str(mod))\n",
    "        try:\n",
    "            results[name] = dir(mod)\n",
    "        except Exception:\n",
    "            results[name] = [\"<ERROR>\"]\n",
    "\n",
    "        if hasattr(mod, \"__path__\"):  # It's a package\n",
    "            for submod_info in pkgutil.iter_modules(mod.__path__):\n",
    "                try:\n",
    "                    submod = importlib.import_module(f\"{mod.__name__}.{submod_info.name}\")\n",
    "                    explore(submod, prefix=\"\")\n",
    "                except Exception as e:\n",
    "                    results[f\"{mod.__name__}.{submod_info.name}\"] = [f\"<IMPORT ERROR: {e}>\"]\n",
    "\n",
    "    explore(module)\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f3d572e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import llama_index\n",
    "# import langchain\n",
    "# print(deep_dir(langchain))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "89a2159a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.service_context_elements.llm_predictor import LLMPredictor\n",
    "from llama_index.core import PromptHelper, ServiceContext\n",
    "from llama_index.cli.rag.base import LLM\n",
    "from llama_index.core.indices import KnowledgeGraphIndex\n",
    "#from llama_index.indices.knowledge_graph.schema import KGTable\n",
    "from llama_index.graph_stores.neo4j import Neo4jPropertyGraphStore\n",
    "from dotenv import load_dotenv\n",
    "from llama_index.core.indices import PropertyGraphIndex\n",
    "import os\n",
    "import requests\n",
    "from neo4j import GraphDatabase\n",
    "import openai\n",
    "from langchain.llms.base import LLM\n",
    "from typing import Optional, List, Mapping, Any\n",
    "from pydantic import BaseModel\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "48ea41f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) On configure openai pour pointer vers Albert\n",
    "openai.api_key = 'sk-eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJ1c2VyX2lkIjo4NDAyLCJ0b2tlbl9pZCI6MTQ4OSwiZXhwaXJlc19hdCI6MTc4MDM1MTIwMH0.mOB9Cx4U4G7K5gin0twePc_WauAEPtRWQ0UaK6oUs9I'\n",
    "openai.api_base = \"https://albert.api.etalab.gouv.fr/v1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9a9f701b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AlbertLLM(LLM, BaseModel):\n",
    "    \"\"\"\n",
    "    Wrapper LangChain pour Albert (API OpenAI-compatible).\n",
    "    \"\"\"\n",
    "\n",
    "    temperature: float = 0.2\n",
    "    model_name: str = \"albert-small\"\n",
    "\n",
    "    class Config:\n",
    "        \"\"\"Pour que pydantic accepte les champs supplémentaires (ignorez-les).\"\"\"\n",
    "        extra = \"ignore\"\n",
    "\n",
    "    def _call(self, prompt: str, stop: Optional[List[str]] = None) -> str:\n",
    "        \"\"\"\n",
    "        Envoie le prompt à l’API Albert et renvoie le texte généré.\n",
    "        \"\"\"\n",
    "        response = openai.ChatCompletion.create(\n",
    "            model=self.model_name,\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            temperature=self.temperature,\n",
    "            stop=stop,\n",
    "            max_tokens=1024,\n",
    "        )\n",
    "        return response.choices[0].message.content\n",
    "\n",
    "    @property\n",
    "    def _identifying_params(self) -> Mapping[str, Any]:\n",
    "        return {\"model_name\": self.model_name, \"temperature\": self.temperature}\n",
    "\n",
    "    @property\n",
    "    def _llm_type(self) -> str:\n",
    "        return \"albert\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ce01de8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\maadt\\AppData\\Local\\Temp\\ipykernel_4384\\1334242705.py:9: DeprecationWarning: Retrievers must implement abstract `_get_relevant_documents` method instead of `get_relevant_documents`\n",
      "  class Neo4jRetriever(BaseRetriever):\n",
      "C:\\Users\\maadt\\AppData\\Local\\Temp\\ipykernel_4384\\1334242705.py:9: DeprecationWarning: Retrievers must implement abstract `_aget_relevant_documents` method instead of `aget_relevant_documents`\n",
      "  class Neo4jRetriever(BaseRetriever):\n"
     ]
    }
   ],
   "source": [
    "# neo4j_retriever.py\n",
    "\n",
    "import os\n",
    "from typing import List, Any\n",
    "from neo4j import GraphDatabase\n",
    "from langchain.schema import BaseRetriever, Document\n",
    "from pydantic import PrivateAttr\n",
    "\n",
    "class Neo4jRetriever(BaseRetriever):\n",
    "    \"\"\"\n",
    "    Retriever LangChain qui interroge Neo4j pour ramener un sous-graphe pertinent.\n",
    "    \"\"\"\n",
    "\n",
    "    # Déclare driver comme attribut privé afin que Pydantic ne l'exige pas comme champ\n",
    "    _driver: Any = PrivateAttr()\n",
    "\n",
    "    def __init__(self):\n",
    "        # Appelle le constructeur de BaseModel\n",
    "        super().__init__()\n",
    "\n",
    "        uri = os.getenv(\"NEO4J_URI\")\n",
    "        user = os.getenv(\"NEO4J_USER\")\n",
    "        pwd  = os.getenv(\"NEO4J_PWD\")\n",
    "        self._driver = GraphDatabase.driver(uri, auth=(user, pwd))\n",
    "\n",
    "    def get_relevant_documents(self, query: str) -> List[Document]:\n",
    "        \"\"\"\n",
    "        1) On extrait des tokens (mots) de la question,\n",
    "        2) On interroge Neo4j pour chaque token correspondant\n",
    "           à un nœud Entity.name,\n",
    "        3) On construit un Document par relation trouvée.\n",
    "        \"\"\"\n",
    "        # Tokenisation basique ; dans la vraie vie, on ferait un NER ou des lowercase+strip\n",
    "        tokens = [tok.strip() for tok in query.split() if len(tok) > 1]\n",
    "        seen_relations = set()\n",
    "        docs: List[Document] = []\n",
    "\n",
    "        with self._driver.session() as session:\n",
    "            for tok in tokens:\n",
    "                cypher = \"\"\"\n",
    "                MATCH (n:Entity {name: $name})-[r]-(m:Entity)\n",
    "                RETURN n.name AS source, type(r) AS rel, m.name AS target\n",
    "                \"\"\"\n",
    "                result = session.run(cypher, name=tok)\n",
    "                for record in result:\n",
    "                    src = record[\"source\"]\n",
    "                    rel = record[\"rel\"]\n",
    "                    tgt = record[\"target\"]\n",
    "                    triple_text = f\"{src} {rel} {tgt}.\"\n",
    "                    if triple_text not in seen_relations:\n",
    "                        seen_relations.add(triple_text)\n",
    "                        docs.append(Document(page_content=triple_text))\n",
    "\n",
    "        return docs\n",
    "\n",
    "    async def aget_relevant_documents(self, query: str) -> List[Document]:\n",
    "        # Pour la plupart des usages on peut renvoyer synchrone\n",
    "        return self.get_relevant_documents(query)\n",
    "\n",
    "    def __del__(self):\n",
    "        try:\n",
    "            self._driver.close()\n",
    "        except:\n",
    "            pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c187ca50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Chat GraphRAG (Neo4j + Albert via LangChain) ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\maadt\\AppData\\Local\\Temp\\ipykernel_4384\\4025754582.py:63: LangChainDeprecationWarning: The method `Chain.__call__` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  result = qa_chain({\"query\": question})\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📝 Réponse :\n",
      "Je suis prêt à répondre à votre question. Quelle est la question ?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Received notification from DBMS server: {severity: WARNING} {code: Neo.ClientNotification.Statement.UnknownLabelWarning} {category: UNRECOGNIZED} {title: The provided label is not in the database.} {description: One of the labels in your query is not available in the database, make sure you didn't misspell it or that the label is available when you run this statement in your application (the missing label name is: Entity)} {position: line: 2, column: 26, offset: 26} for query: '\\n                MATCH (n:Entity {name: $name})-[r]-(m:Entity)\\n                RETURN n.name AS source, type(r) AS rel, m.name AS target\\n                '\n",
      "Received notification from DBMS server: {severity: WARNING} {code: Neo.ClientNotification.Statement.UnknownLabelWarning} {category: UNRECOGNIZED} {title: The provided label is not in the database.} {description: One of the labels in your query is not available in the database, make sure you didn't misspell it or that the label is available when you run this statement in your application (the missing label name is: Entity)} {position: line: 2, column: 55, offset: 55} for query: '\\n                MATCH (n:Entity {name: $name})-[r]-(m:Entity)\\n                RETURN n.name AS source, type(r) AS rel, m.name AS target\\n                '\n",
      "Received notification from DBMS server: {severity: WARNING} {code: Neo.ClientNotification.Statement.UnknownLabelWarning} {category: UNRECOGNIZED} {title: The provided label is not in the database.} {description: One of the labels in your query is not available in the database, make sure you didn't misspell it or that the label is available when you run this statement in your application (the missing label name is: Entity)} {position: line: 2, column: 26, offset: 26} for query: '\\n                MATCH (n:Entity {name: $name})-[r]-(m:Entity)\\n                RETURN n.name AS source, type(r) AS rel, m.name AS target\\n                '\n",
      "Received notification from DBMS server: {severity: WARNING} {code: Neo.ClientNotification.Statement.UnknownLabelWarning} {category: UNRECOGNIZED} {title: The provided label is not in the database.} {description: One of the labels in your query is not available in the database, make sure you didn't misspell it or that the label is available when you run this statement in your application (the missing label name is: Entity)} {position: line: 2, column: 55, offset: 55} for query: '\\n                MATCH (n:Entity {name: $name})-[r]-(m:Entity)\\n                RETURN n.name AS source, type(r) AS rel, m.name AS target\\n                '\n",
      "Received notification from DBMS server: {severity: WARNING} {code: Neo.ClientNotification.Statement.UnknownLabelWarning} {category: UNRECOGNIZED} {title: The provided label is not in the database.} {description: One of the labels in your query is not available in the database, make sure you didn't misspell it or that the label is available when you run this statement in your application (the missing label name is: Entity)} {position: line: 2, column: 26, offset: 26} for query: '\\n                MATCH (n:Entity {name: $name})-[r]-(m:Entity)\\n                RETURN n.name AS source, type(r) AS rel, m.name AS target\\n                '\n",
      "Received notification from DBMS server: {severity: WARNING} {code: Neo.ClientNotification.Statement.UnknownLabelWarning} {category: UNRECOGNIZED} {title: The provided label is not in the database.} {description: One of the labels in your query is not available in the database, make sure you didn't misspell it or that the label is available when you run this statement in your application (the missing label name is: Entity)} {position: line: 2, column: 55, offset: 55} for query: '\\n                MATCH (n:Entity {name: $name})-[r]-(m:Entity)\\n                RETURN n.name AS source, type(r) AS rel, m.name AS target\\n                '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📝 Réponse :\n",
      "Désolé, je n'ai pas cette information.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Received notification from DBMS server: {severity: WARNING} {code: Neo.ClientNotification.Statement.UnknownLabelWarning} {category: UNRECOGNIZED} {title: The provided label is not in the database.} {description: One of the labels in your query is not available in the database, make sure you didn't misspell it or that the label is available when you run this statement in your application (the missing label name is: Entity)} {position: line: 2, column: 26, offset: 26} for query: '\\n                MATCH (n:Entity {name: $name})-[r]-(m:Entity)\\n                RETURN n.name AS source, type(r) AS rel, m.name AS target\\n                '\n",
      "Received notification from DBMS server: {severity: WARNING} {code: Neo.ClientNotification.Statement.UnknownLabelWarning} {category: UNRECOGNIZED} {title: The provided label is not in the database.} {description: One of the labels in your query is not available in the database, make sure you didn't misspell it or that the label is available when you run this statement in your application (the missing label name is: Entity)} {position: line: 2, column: 55, offset: 55} for query: '\\n                MATCH (n:Entity {name: $name})-[r]-(m:Entity)\\n                RETURN n.name AS source, type(r) AS rel, m.name AS target\\n                '\n",
      "Received notification from DBMS server: {severity: WARNING} {code: Neo.ClientNotification.Statement.UnknownLabelWarning} {category: UNRECOGNIZED} {title: The provided label is not in the database.} {description: One of the labels in your query is not available in the database, make sure you didn't misspell it or that the label is available when you run this statement in your application (the missing label name is: Entity)} {position: line: 2, column: 26, offset: 26} for query: '\\n                MATCH (n:Entity {name: $name})-[r]-(m:Entity)\\n                RETURN n.name AS source, type(r) AS rel, m.name AS target\\n                '\n",
      "Received notification from DBMS server: {severity: WARNING} {code: Neo.ClientNotification.Statement.UnknownLabelWarning} {category: UNRECOGNIZED} {title: The provided label is not in the database.} {description: One of the labels in your query is not available in the database, make sure you didn't misspell it or that the label is available when you run this statement in your application (the missing label name is: Entity)} {position: line: 2, column: 55, offset: 55} for query: '\\n                MATCH (n:Entity {name: $name})-[r]-(m:Entity)\\n                RETURN n.name AS source, type(r) AS rel, m.name AS target\\n                '\n",
      "Received notification from DBMS server: {severity: WARNING} {code: Neo.ClientNotification.Statement.UnknownLabelWarning} {category: UNRECOGNIZED} {title: The provided label is not in the database.} {description: One of the labels in your query is not available in the database, make sure you didn't misspell it or that the label is available when you run this statement in your application (the missing label name is: Entity)} {position: line: 2, column: 26, offset: 26} for query: '\\n                MATCH (n:Entity {name: $name})-[r]-(m:Entity)\\n                RETURN n.name AS source, type(r) AS rel, m.name AS target\\n                '\n",
      "Received notification from DBMS server: {severity: WARNING} {code: Neo.ClientNotification.Statement.UnknownLabelWarning} {category: UNRECOGNIZED} {title: The provided label is not in the database.} {description: One of the labels in your query is not available in the database, make sure you didn't misspell it or that the label is available when you run this statement in your application (the missing label name is: Entity)} {position: line: 2, column: 55, offset: 55} for query: '\\n                MATCH (n:Entity {name: $name})-[r]-(m:Entity)\\n                RETURN n.name AS source, type(r) AS rel, m.name AS target\\n                '\n",
      "Received notification from DBMS server: {severity: WARNING} {code: Neo.ClientNotification.Statement.UnknownLabelWarning} {category: UNRECOGNIZED} {title: The provided label is not in the database.} {description: One of the labels in your query is not available in the database, make sure you didn't misspell it or that the label is available when you run this statement in your application (the missing label name is: Entity)} {position: line: 2, column: 26, offset: 26} for query: '\\n                MATCH (n:Entity {name: $name})-[r]-(m:Entity)\\n                RETURN n.name AS source, type(r) AS rel, m.name AS target\\n                '\n",
      "Received notification from DBMS server: {severity: WARNING} {code: Neo.ClientNotification.Statement.UnknownLabelWarning} {category: UNRECOGNIZED} {title: The provided label is not in the database.} {description: One of the labels in your query is not available in the database, make sure you didn't misspell it or that the label is available when you run this statement in your application (the missing label name is: Entity)} {position: line: 2, column: 55, offset: 55} for query: '\\n                MATCH (n:Entity {name: $name})-[r]-(m:Entity)\\n                RETURN n.name AS source, type(r) AS rel, m.name AS target\\n                '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📝 Réponse :\n",
      "Je connais la MAcif. La MAcif est un projet de véhicule autonome développé par le groupe PSA (Peugeot-Citroën) en collaboration avec l'Institut national de recherche en informatique et en automatique (INRIA) et l'Université de Nice Sophia Antipolis. Le but de ce projet est de développer un véhicule autonome capable de conduire de manière sécuritaire et efficace dans des conditions réelles.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Received notification from DBMS server: {severity: WARNING} {code: Neo.ClientNotification.Statement.UnknownLabelWarning} {category: UNRECOGNIZED} {title: The provided label is not in the database.} {description: One of the labels in your query is not available in the database, make sure you didn't misspell it or that the label is available when you run this statement in your application (the missing label name is: Entity)} {position: line: 2, column: 26, offset: 26} for query: '\\n                MATCH (n:Entity {name: $name})-[r]-(m:Entity)\\n                RETURN n.name AS source, type(r) AS rel, m.name AS target\\n                '\n",
      "Received notification from DBMS server: {severity: WARNING} {code: Neo.ClientNotification.Statement.UnknownLabelWarning} {category: UNRECOGNIZED} {title: The provided label is not in the database.} {description: One of the labels in your query is not available in the database, make sure you didn't misspell it or that the label is available when you run this statement in your application (the missing label name is: Entity)} {position: line: 2, column: 55, offset: 55} for query: '\\n                MATCH (n:Entity {name: $name})-[r]-(m:Entity)\\n                RETURN n.name AS source, type(r) AS rel, m.name AS target\\n                '\n",
      "Received notification from DBMS server: {severity: WARNING} {code: Neo.ClientNotification.Statement.UnknownLabelWarning} {category: UNRECOGNIZED} {title: The provided label is not in the database.} {description: One of the labels in your query is not available in the database, make sure you didn't misspell it or that the label is available when you run this statement in your application (the missing label name is: Entity)} {position: line: 2, column: 26, offset: 26} for query: '\\n                MATCH (n:Entity {name: $name})-[r]-(m:Entity)\\n                RETURN n.name AS source, type(r) AS rel, m.name AS target\\n                '\n",
      "Received notification from DBMS server: {severity: WARNING} {code: Neo.ClientNotification.Statement.UnknownLabelWarning} {category: UNRECOGNIZED} {title: The provided label is not in the database.} {description: One of the labels in your query is not available in the database, make sure you didn't misspell it or that the label is available when you run this statement in your application (the missing label name is: Entity)} {position: line: 2, column: 55, offset: 55} for query: '\\n                MATCH (n:Entity {name: $name})-[r]-(m:Entity)\\n                RETURN n.name AS source, type(r) AS rel, m.name AS target\\n                '\n",
      "Received notification from DBMS server: {severity: WARNING} {code: Neo.ClientNotification.Statement.UnknownLabelWarning} {category: UNRECOGNIZED} {title: The provided label is not in the database.} {description: One of the labels in your query is not available in the database, make sure you didn't misspell it or that the label is available when you run this statement in your application (the missing label name is: Entity)} {position: line: 2, column: 26, offset: 26} for query: '\\n                MATCH (n:Entity {name: $name})-[r]-(m:Entity)\\n                RETURN n.name AS source, type(r) AS rel, m.name AS target\\n                '\n",
      "Received notification from DBMS server: {severity: WARNING} {code: Neo.ClientNotification.Statement.UnknownLabelWarning} {category: UNRECOGNIZED} {title: The provided label is not in the database.} {description: One of the labels in your query is not available in the database, make sure you didn't misspell it or that the label is available when you run this statement in your application (the missing label name is: Entity)} {position: line: 2, column: 55, offset: 55} for query: '\\n                MATCH (n:Entity {name: $name})-[r]-(m:Entity)\\n                RETURN n.name AS source, type(r) AS rel, m.name AS target\\n                '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📝 Réponse :\n",
      "Désolé, je n'ai pas cette information.\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.indexes.vectorstore import RetrievalQA\n",
    "# ----------------------------------------\n",
    "# 1. Charger les variables d’environnement\n",
    "# ----------------------------------------\n",
    "load_dotenv()\n",
    "\n",
    "# ----------------------------------------\n",
    "# 2. Instancier le LLM Albert\n",
    "# ----------------------------------------\n",
    "llm = AlbertLLM(temperature=0.2)\n",
    "\n",
    "# ----------------------------------------\n",
    "# 3. Créer le prompt template pour la QA\n",
    "# ----------------------------------------\n",
    "# {context} = textes du graphe Neo4j\n",
    "# {question} = question utilisateur\n",
    "prompt_template = \"\"\"\n",
    "Tu es un assistant expert en véhicules autonomes.\n",
    "Voici le contexte extrait d'un graphe de connaissances :\n",
    "{context}\n",
    "\n",
    "Question : {question}\n",
    "\n",
    "Réponds de façon précise, en t’appuyant seulement sur ces faits. Si ce n’est pas dans le contexte, répond « Désolé, je n’ai pas cette information. ».\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=prompt_template,\n",
    "    input_variables=[\"context\", \"question\"]\n",
    ")\n",
    "\n",
    "# ----------------------------------------\n",
    "# 4. Instancier le retriever Neo4j personnalisé\n",
    "# ----------------------------------------\n",
    "graph_retriever = Neo4jRetriever()\n",
    "\n",
    "# ----------------------------------------\n",
    "# 5. Construire la chaîne RetrievalQA\n",
    "# ----------------------------------------\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",           # on bourre tout le contexte d’un coup\n",
    "    retriever=graph_retriever,\n",
    "    return_source_documents=False,\n",
    "    chain_type_kwargs={\"prompt\": prompt}\n",
    ")\n",
    "\n",
    "# ----------------------------------------\n",
    "# 6. Boucle interactive\n",
    "# ----------------------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"=== Chat GraphRAG (Neo4j + Albert via LangChain) ===\")\n",
    "    while True:\n",
    "        question = input(\"\\nPose ta question (ou « exit » pour quitter) : \")\n",
    "        if question.lower().strip() in (\"exit\", \"quit\"):\n",
    "            break\n",
    "\n",
    "        # LangChain :\n",
    "        #  1) graph_retriever.get_relevant_documents(question) → liste de Docs\n",
    "        #  2) Concatène “context” = sommaire des docs, plus “question” dans le prompt\n",
    "        #  3) Envoie tout à llm._call(prompt_final) → Albert → génération\n",
    "        result = qa_chain({\"query\": question})\n",
    "        print(\"\\n📝 Réponse :\")\n",
    "        print(result[\"result\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b350558",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# ----------------------------------------\n",
    "# 2. Définir une classe AlbertLLM qui implémente LLM de llama-index\n",
    "# ----------------------------------------\n",
    "class AlbertLLM(LLM):\n",
    "    \"\"\"\n",
    "    Implémentation d'un LLM compatible LlamaIndex, pointant vers Albert API.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, temperature: float = 0.2):\n",
    "        self.temperature = temperature\n",
    "\n",
    "    @property\n",
    "    def metadata(self) -> dict:\n",
    "        # Quelques infos génériques ; on peut ajuster si besoin\n",
    "        return {\"model_name\": \"gpt-3.5-turbo\", \"max_tokens\": 2048}\n",
    "\n",
    "    def _call(self, prompt: str, **kwargs) -> str:\n",
    "        \"\"\"\n",
    "        Envoie le prompt à l’API Albert (endpoint OpenAI-compatible) et retourne la réponse.\n",
    "        \"\"\"\n",
    "        url = \"https://albert.api.etalab.gouv.fr/v1/chat/completions\"\n",
    "        headers = {\n",
    "            \"Authorization\": f\"Bearer {albert_api_key}\",\n",
    "            \"Content-Type\": \"application/json\"\n",
    "        }\n",
    "        json_data = {\n",
    "            \"model\": \"gpt-3.5-turbo\",\n",
    "            \"messages\": [{\"role\": \"user\", \"content\": prompt}],\n",
    "            \"temperature\": self.temperature,\n",
    "            # Vous pouvez ajouter \"max_tokens\": 1024, etc., si besoin\n",
    "        }\n",
    "        resp = requests.post(url, headers=headers, json=json_data)\n",
    "        resp.raise_for_status()\n",
    "        data = resp.json()\n",
    "        return data[\"choices\"][0][\"message\"][\"content\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89f54c16",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'BaseLLMPredictor' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[64]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# ----------------------------------------\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m# 3. Définir un « Predictor » qui implémente BaseLLMPredictor\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# ----------------------------------------\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mAlbertLLMPredictor\u001b[39;00m(\u001b[43mBaseLLMPredictor\u001b[49m):\n\u001b[32m      5\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m      6\u001b[39m \u001b[33;03m    Wrapper autour d'AlbertLLM pour respecter l'interface BaseLLMPredictor\u001b[39;00m\n\u001b[32m      7\u001b[39m \u001b[33;03m    (v0.12 de llama-index).\u001b[39;00m\n\u001b[32m      8\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m     10\u001b[39m     \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, llm: AlbertLLM):\n",
      "\u001b[31mNameError\u001b[39m: name 'BaseLLMPredictor' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "# ----------------------------------------\n",
    "# 3. Définir un « Predictor » qui implémente BaseLLMPredictor\n",
    "# ----------------------------------------\n",
    "class AlbertLLMPredictor(BaseLLMPredictor):\n",
    "    \"\"\"\n",
    "    Wrapper autour d'AlbertLLM pour respecter l'interface BaseLLMPredictor\n",
    "    (v0.12 de llama-index).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, llm: AlbertLLM):\n",
    "        self._llm = llm\n",
    "        self._callback_manager = CallbackManager()\n",
    "\n",
    "    @property\n",
    "    def llm(self) -> LLM:\n",
    "        return self._llm\n",
    "\n",
    "    @property\n",
    "    def callback_manager(self) -> CallbackManager:\n",
    "        return self._callback_manager\n",
    "\n",
    "    @property\n",
    "    def metadata(self) -> LLMMetadata:\n",
    "        return self._llm.metadata\n",
    "\n",
    "    def predict(self, prompt: BasePromptTemplate, **prompt_args) -> str:\n",
    "        \"\"\"\n",
    "        Construit une chaîne de caractères à partir du BasePromptTemplate + args,\n",
    "        puis appelle le LLM pour obtenir la réponse complète.\n",
    "        \"\"\"\n",
    "        # prompt.format(prompt_args) renvoie la chaîne textuelle finale\n",
    "        text = prompt.format(**prompt_args)\n",
    "        return self._llm._call(text)\n",
    "\n",
    "    def stream(self, prompt: BasePromptTemplate, **prompt_args) -> TokenGen:\n",
    "        \"\"\"\n",
    "        Streaming non implémenté (renvoie tout en une fois).\n",
    "        \"\"\"\n",
    "        text = prompt.format(**prompt_args)\n",
    "        # Pour un vrai streaming, il faudrait appeler l’API Albert en mode stream\n",
    "        # et yield() chaque token, mais ici on renvoie tout d’un coup :\n",
    "        yield self._llm._call(text)\n",
    "\n",
    "    async def apredict(self, prompt: BasePromptTemplate, **prompt_args) -> str:\n",
    "        \"\"\"\n",
    "        Async predict non implémenté : appelle _call synchronously pour l’instant.\n",
    "        \"\"\"\n",
    "        text = prompt.format(**prompt_args)\n",
    "        return self._llm._call(text)\n",
    "\n",
    "    async def astream(self, prompt: BasePromptTemplate, **prompt_args) -> TokenAsyncGen:\n",
    "        \"\"\"\n",
    "        Async streaming non implémenté : renvoie tout d’un coup.\n",
    "        \"\"\"\n",
    "        text = prompt.format(**prompt_args)\n",
    "        yield self._llm._call(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e604f28",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Can't instantiate abstract class AlbertLLM without an implementation for abstract methods 'achat', 'acomplete', 'astream_chat', 'astream_complete', 'chat', 'complete', 'stream_chat', 'stream_complete'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[63]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# ----------------------------------------\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m# 3. Configurer le LLM Predictor avec AlbertLLM\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# ----------------------------------------\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m llm = \u001b[43mAlbertLLM\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.2\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      5\u001b[39m prompt_helper = PromptHelper()\n\u001b[32m      7\u001b[39m service_context = ServiceContext.from_defaults(\n\u001b[32m      8\u001b[39m     llm=llm,\n\u001b[32m      9\u001b[39m     prompt_helper=prompt_helper\n\u001b[32m     10\u001b[39m )\n",
      "\u001b[31mTypeError\u001b[39m: Can't instantiate abstract class AlbertLLM without an implementation for abstract methods 'achat', 'acomplete', 'astream_chat', 'astream_complete', 'chat', 'complete', 'stream_chat', 'stream_complete'"
     ]
    }
   ],
   "source": [
    "# ----------------------------------------\n",
    "# 3. Configurer le LLM Predictor avec AlbertLLM\n",
    "# ----------------------------------------\n",
    "llm = AlbertLLM(temperature=0.2)\n",
    "prompt_helper = PromptHelper()\n",
    "\n",
    "service_context = ServiceContext.from_defaults(\n",
    "    llm=llm,\n",
    "    prompt_helper=prompt_helper\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06e878a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------\n",
    "# 4. Construire l’index GraphRAG avec LlamaIndex\n",
    "# ----------------------------------------\n",
    "graph_index = KnowledgeGraphIndex.from_dicts(\n",
    "    all_triples,\n",
    "    service_context=service_context\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b8c5928",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------\n",
    "# 5. Fonction de réponse via GraphRAG + Albert\n",
    "# ----------------------------------------\n",
    "def answer_from_graph_index(question: str) -> str:\n",
    "    \"\"\"\n",
    "    Interroge le KnowledgeGraphIndex. LlamaIndex parcourt le graphe interne\n",
    "    et génère la réponse en appelant AlbertLLM pour la complétion finale.\n",
    "    \"\"\"\n",
    "    response = graph_index.query(question)\n",
    "    return response.response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daa0ee4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------\n",
    "# 6. Exemple d’utilisation\n",
    "# ----------------------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    q = \"Que vise la MACIF dans le domaine du véhicule autonome ?\"\n",
    "    answer = answer_from_graph_index(q)\n",
    "    print(\"Question :\", q)\n",
    "    print(\"Réponse :\", answer)\n",
    "\n",
    "    driver.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73da0326",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# 1. Transformer les triples en texte\n",
    "triples_text = \"\\n\".join(\n",
    "    f\"{t['source']} {t['relation']} {t['target']}.\" for t in all_triples\n",
    ")\n",
    "\n",
    "# 2. Créer un document\n",
    "documents = [Document(triples_text)]\n",
    "\n",
    "# 3. Parser les documents\n",
    "node_parser = SimpleNodeParser()\n",
    "nodes = node_parser.get_nodes_from_documents(documents)\n",
    "\n",
    "# 4. Créer un graph store en mémoire\n",
    "graph_store = SimpleGraphStore()\n",
    "\n",
    "# 5. Construire l’index KnowledgeGraphIndex\n",
    "service_context = ServiceContext.from_defaults(llm=OpenAI(temperature=0))\n",
    "kg_index = KnowledgeGraphIndex(\n",
    "    nodes=nodes,\n",
    "    graph_store=graph_store,\n",
    "    service_context=service_context\n",
    ")\n",
    "\n",
    "# 6. Interroger l’index\n",
    "query = \"Quels capteurs sont utilisés par Tesla ?\"\n",
    "response = kg_index.query(query)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e2f0387",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "type object 'KnowledgeGraphIndex' has no attribute 'from_dicts'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# ----------------------------------------\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m# 4. Construire l’index GraphRAG avec LlamaIndex\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# ----------------------------------------\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# Ici, on crée un KnowledgeGraphIndex directement à partir de la liste de dicts.\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m graph_index = \u001b[43mKnowledgeGraphIndex\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_dicts\u001b[49m(\n\u001b[32m      6\u001b[39m     all_triples,\n\u001b[32m      7\u001b[39m     service_context=service_context\n\u001b[32m      8\u001b[39m )\n",
      "\u001b[31mAttributeError\u001b[39m: type object 'KnowledgeGraphIndex' has no attribute 'from_dicts'"
     ]
    }
   ],
   "source": [
    "# ----------------------------------------\n",
    "# 4. Construire l’index GraphRAG avec LlamaIndex\n",
    "# ----------------------------------------\n",
    "# Ici, on crée un KnowledgeGraphIndex directement à partir de la liste de dicts.\n",
    "graph_index = KnowledgeGraphIndex.from_dicts(\n",
    "    all_triples,\n",
    "    service_context=service_context\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e5840ec",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'llama_index.indices.knowledge_graph'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m      5\u001b[39m     \u001b[38;5;66;03m# Pour LlamaIndex 0.8.x\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mllama_index\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mindices\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mknowledge_graph\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mbase\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m KGTable, KnowledgeGraphIndex\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n\u001b[32m      8\u001b[39m     \u001b[38;5;66;03m# Pour LlamaIndex 0.9+ (chemin possible)\u001b[39;00m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'llama_index.indices.knowledge_graph'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 9\u001b[39m\n\u001b[32m      6\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mllama_index\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mindices\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mknowledge_graph\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mbase\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m KGTable, KnowledgeGraphIndex\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n\u001b[32m      8\u001b[39m     \u001b[38;5;66;03m# Pour LlamaIndex 0.9+ (chemin possible)\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mllama_index\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mindices\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mknowledge_graph\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mschema\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m KGTable\n\u001b[32m     10\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mllama_index\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mindices\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mknowledge_graph\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mbase\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m KnowledgeGraphIndex\n\u001b[32m     12\u001b[39m \u001b[38;5;66;03m# 2) Construire la table à partir d'une liste de dicts\u001b[39;00m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'llama_index.indices.knowledge_graph'"
     ]
    }
   ],
   "source": [
    "# import llama_index\n",
    "# # llamaindex_demo.py (suite)\n",
    "# # 1) Import du KGTable et KnowledgeGraphIndex\n",
    "# try:\n",
    "#     # Pour LlamaIndex 0.8.x\n",
    "#     from llama_index.indices.knowledge_graph.base import KGTable, KnowledgeGraphIndex\n",
    "# except ImportError:\n",
    "#     # Pour LlamaIndex 0.9+ (chemin possible)\n",
    "#     from llama_index.indices.knowledge_graph.schema import KGTable\n",
    "#     from llama_index.indices.knowledge_graph.base import KnowledgeGraphIndex\n",
    "\n",
    "# # 2) Construire la table à partir d'une liste de dicts\n",
    "# kg_table = KGTable.from_list(all_triples)\n",
    "\n",
    "# # 3) Reprendre le service_context déjà configuré plus haut\n",
    "# #    (llm_predictor + prompt_helper)\n",
    "\n",
    "# # 4) Créer l'index GraphRAG à partir du KGTable\n",
    "# graph_index = KnowledgeGraphIndex.from_kg_table(\n",
    "#     kg_table,\n",
    "#     service_context=service_context\n",
    "# )\n",
    "\n",
    "# # 5) Exemple de requête GraphRAG\n",
    "# def answer_from_graph_index(question: str) -> str:\n",
    "#     response = graph_index.query(question)\n",
    "#     return response.response\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     q = \"Que vise la MACIF dans le domaine du véhicule autonome ?\"\n",
    "#     print(answer_from_graph_index(q))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
