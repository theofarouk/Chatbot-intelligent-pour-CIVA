{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "624fea1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(\"outputs/all_triples2.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    all_triples = json.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "be4d7f46",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\maadt\\AppData\\Local\\Temp\\ipykernel_4384\\773710034.py:41: DeprecationWarning: write_transaction has been renamed to execute_write\n",
      "  session.write_transaction(insert_triplet, src, rel, tgt)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ 493 triplets ins√©r√©s dans Neo4j.\n"
     ]
    }
   ],
   "source": [
    "from neo4j import GraphDatabase\n",
    "\n",
    "# ---------- 1. Configuration de la connexion Neo4j ----------\n",
    "# Remplacez ces valeurs par celles de votre instance Neo4j\n",
    "uri  = \"bolt://localhost:7687\"\n",
    "user = \"neo4j\"\n",
    "pwd  = \"BNEOsucks8921_\"\n",
    "\n",
    "# Cr√©e un driver Neo4j\n",
    "driver = GraphDatabase.driver(uri, auth=(user, pwd))\n",
    "\n",
    "\n",
    "# ---------- 2. Fonction d‚Äôinsertion d‚Äôun triplet dans Neo4j ----------\n",
    "def insert_triplet(tx, source, relation, target):\n",
    "    \"\"\"\n",
    "    Cr√©e ou r√©cup√®re deux n≈ìuds (Entity) nomm√©s 'source' et 'target', \n",
    "    puis cr√©e (si n√©cessaire) la relation entre eux.\n",
    "    Le type de relation est stock√© dans la propri√©t√© 'type' de l'arc.\n",
    "\n",
    "    Exemple Cypher g√©n√©r√© :\n",
    "      MERGE (a:Entity {name: $source})\n",
    "      MERGE (b:Entity {name: $target})\n",
    "      MERGE (a)-[:RELATION {type: $relation}]->(b)\n",
    "    \"\"\"\n",
    "    query = \"\"\"\n",
    "    MERGE (a:Source {name: $source})\n",
    "    MERGE (b:Target {name: $target})\n",
    "    MERGE (a)-[:RELATION {type: $relation}]->(b)\n",
    "    \"\"\"\n",
    "    tx.run(query, source=source, relation=relation, target=target)\n",
    "\n",
    "\n",
    "# ---------- 3. Fonction de chargement de tous les triplets ----------\n",
    "def load_all_triplets(triplets):\n",
    "    \"\"\"\n",
    "    Parcourt la liste 'triplets' (liste de tuples (source, relation, target))\n",
    "    et ex√©cute 'insert_triplet' pour chacun d‚Äôentre eux dans une m√™me session.\n",
    "    \"\"\"\n",
    "    with driver.session() as session:\n",
    "        for src, rel, tgt in triplets:\n",
    "            session.write_transaction(insert_triplet, src, rel, tgt)\n",
    "    print(f\"‚úÖ {len(triplets)} triplets ins√©r√©s dans Neo4j.\")\n",
    "\n",
    "\n",
    "# ---------- 4. Point d‚Äôentr√©e du script ----------\n",
    "if __name__ == \"__main__\":\n",
    "    # Supposons que vous ayez, dans un autre module, votre liste 'all_triples'\n",
    "    # Exemple : all_triples = [(\"Navette autonome\", \"test√©e dans\", \"Toulouse\"), ...]\n",
    "    # Il faut donc importer ou recr√©er cette liste ici.\n",
    "    #\n",
    "    # Si votre extraction a produit un module Python ou un fichier pickle,\n",
    "    # r√©cup√©rez la liste de tuples et assignez-la √† 'all_triples'.\n",
    "    #\n",
    "    # Par exemple, si vous avez sauvegard√© vos relations dans un fichier JSON :\n",
    "    #\n",
    "    # import json\n",
    "    # with open(\"outputs/all_triples.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    #     all_triples = json.load(f)\n",
    "    #\n",
    "    # Mais ici, illustrons un exemple statique (√† remplacer par votre propre liste) :\n",
    "\n",
    "    # Charge tous les triplets dans Neo4j\n",
    "    load_all_triplets(all_triples)\n",
    "\n",
    "    # Ferme le driver une fois termin√©\n",
    "   # driver.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "67e29929",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_all_triples():\n",
    "    \"\"\"\n",
    "    R√©cup√®re l'ensemble des triplets (entit√©, relation, entit√©) dans Neo4j.\n",
    "    \"\"\"\n",
    "    query = \"\"\"\n",
    "    MATCH (a:Source)-[r:RELATION]->(b:Target)\n",
    "    RETURN a.name AS source, r.type AS relation, b.name AS target\n",
    "    \"\"\"\n",
    "    triples = []\n",
    "    with driver.session() as session:\n",
    "        for record in session.run(query):\n",
    "            triples.append({\n",
    "                \"source\": record[\"source\"],\n",
    "                \"relation\": record[\"relation\"],\n",
    "                \"target\": record[\"target\"]\n",
    "            })\n",
    "    return triples\n",
    "\n",
    "# Exemple :\n",
    "all_triples = fetch_all_triples()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6888c69e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import llama_index\n",
    "# print(dir(llama_index))\n",
    "\n",
    "# import llama_index.core\n",
    "# print(dir(llama_index.core))\n",
    "\n",
    "# import llama_index.core.indices\n",
    "# print(dir(llama_index.core.indices))\n",
    "\n",
    "\n",
    "# import llama_index.indices\n",
    "# print(dir(llama_index.indices))\n",
    "\n",
    "# import llama_index.llms\n",
    "# print(dir(llama_index.llms))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "23761c21",
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import pkgutil\n",
    "\n",
    "def deep_dir(module):\n",
    "    results = {}\n",
    "\n",
    "    def explore(mod, prefix=\"\"):\n",
    "        name = prefix + (mod.__name__ if hasattr(mod, \"__name__\") else str(mod))\n",
    "        try:\n",
    "            results[name] = dir(mod)\n",
    "        except Exception:\n",
    "            results[name] = [\"<ERROR>\"]\n",
    "\n",
    "        if hasattr(mod, \"__path__\"):  # It's a package\n",
    "            for submod_info in pkgutil.iter_modules(mod.__path__):\n",
    "                try:\n",
    "                    submod = importlib.import_module(f\"{mod.__name__}.{submod_info.name}\")\n",
    "                    explore(submod, prefix=\"\")\n",
    "                except Exception as e:\n",
    "                    results[f\"{mod.__name__}.{submod_info.name}\"] = [f\"<IMPORT ERROR: {e}>\"]\n",
    "\n",
    "    explore(module)\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f3d572e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import llama_index\n",
    "# import langchain\n",
    "# print(deep_dir(langchain))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "89a2159a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.service_context_elements.llm_predictor import LLMPredictor\n",
    "from llama_index.core import PromptHelper, ServiceContext\n",
    "from llama_index.cli.rag.base import LLM\n",
    "from llama_index.core.indices import KnowledgeGraphIndex\n",
    "#from llama_index.indices.knowledge_graph.schema import KGTable\n",
    "from llama_index.graph_stores.neo4j import Neo4jPropertyGraphStore\n",
    "from dotenv import load_dotenv\n",
    "from llama_index.core.indices import PropertyGraphIndex\n",
    "import os\n",
    "import requests\n",
    "from neo4j import GraphDatabase\n",
    "import openai\n",
    "from langchain.llms.base import LLM\n",
    "from typing import Optional, List, Mapping, Any\n",
    "from pydantic import BaseModel\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "48ea41f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) On configure openai pour pointer vers Albert\n",
    "openai.api_key = 'sk-eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJ1c2VyX2lkIjo4NDAyLCJ0b2tlbl9pZCI6MTQ4OSwiZXhwaXJlc19hdCI6MTc4MDM1MTIwMH0.mOB9Cx4U4G7K5gin0twePc_WauAEPtRWQ0UaK6oUs9I'\n",
    "openai.api_base = \"https://albert.api.etalab.gouv.fr/v1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9a9f701b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AlbertLLM(LLM, BaseModel):\n",
    "    \"\"\"\n",
    "    Wrapper LangChain pour Albert (API OpenAI-compatible).\n",
    "    \"\"\"\n",
    "\n",
    "    temperature: float = 0.2\n",
    "    model_name: str = \"albert-small\"\n",
    "\n",
    "    class Config:\n",
    "        \"\"\"Pour que pydantic accepte les champs suppl√©mentaires (ignorez-les).\"\"\"\n",
    "        extra = \"ignore\"\n",
    "\n",
    "    def _call(self, prompt: str, stop: Optional[List[str]] = None) -> str:\n",
    "        \"\"\"\n",
    "        Envoie le prompt √† l‚ÄôAPI Albert et renvoie le texte g√©n√©r√©.\n",
    "        \"\"\"\n",
    "        response = openai.ChatCompletion.create(\n",
    "            model=self.model_name,\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            temperature=self.temperature,\n",
    "            stop=stop,\n",
    "            max_tokens=1024,\n",
    "        )\n",
    "        return response.choices[0].message.content\n",
    "\n",
    "    @property\n",
    "    def _identifying_params(self) -> Mapping[str, Any]:\n",
    "        return {\"model_name\": self.model_name, \"temperature\": self.temperature}\n",
    "\n",
    "    @property\n",
    "    def _llm_type(self) -> str:\n",
    "        return \"albert\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ce01de8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\maadt\\AppData\\Local\\Temp\\ipykernel_4384\\1334242705.py:9: DeprecationWarning: Retrievers must implement abstract `_get_relevant_documents` method instead of `get_relevant_documents`\n",
      "  class Neo4jRetriever(BaseRetriever):\n",
      "C:\\Users\\maadt\\AppData\\Local\\Temp\\ipykernel_4384\\1334242705.py:9: DeprecationWarning: Retrievers must implement abstract `_aget_relevant_documents` method instead of `aget_relevant_documents`\n",
      "  class Neo4jRetriever(BaseRetriever):\n"
     ]
    }
   ],
   "source": [
    "# neo4j_retriever.py\n",
    "\n",
    "import os\n",
    "from typing import List, Any\n",
    "from neo4j import GraphDatabase\n",
    "from langchain.schema import BaseRetriever, Document\n",
    "from pydantic import PrivateAttr\n",
    "\n",
    "class Neo4jRetriever(BaseRetriever):\n",
    "    \"\"\"\n",
    "    Retriever LangChain qui interroge Neo4j pour ramener un sous-graphe pertinent.\n",
    "    \"\"\"\n",
    "\n",
    "    # D√©clare driver comme attribut priv√© afin que Pydantic ne l'exige pas comme champ\n",
    "    _driver: Any = PrivateAttr()\n",
    "\n",
    "    def __init__(self):\n",
    "        # Appelle le constructeur de BaseModel\n",
    "        super().__init__()\n",
    "\n",
    "        uri = os.getenv(\"NEO4J_URI\")\n",
    "        user = os.getenv(\"NEO4J_USER\")\n",
    "        pwd  = os.getenv(\"NEO4J_PWD\")\n",
    "        self._driver = GraphDatabase.driver(uri, auth=(user, pwd))\n",
    "\n",
    "    def get_relevant_documents(self, query: str) -> List[Document]:\n",
    "        \"\"\"\n",
    "        1) On extrait des tokens (mots) de la question,\n",
    "        2) On interroge Neo4j pour chaque token correspondant\n",
    "           √† un n≈ìud Entity.name,\n",
    "        3) On construit un Document par relation trouv√©e.\n",
    "        \"\"\"\n",
    "        # Tokenisation basique ; dans la vraie vie, on ferait un NER ou des lowercase+strip\n",
    "        tokens = [tok.strip() for tok in query.split() if len(tok) > 1]\n",
    "        seen_relations = set()\n",
    "        docs: List[Document] = []\n",
    "\n",
    "        with self._driver.session() as session:\n",
    "            for tok in tokens:\n",
    "                cypher = \"\"\"\n",
    "                MATCH (n:Entity {name: $name})-[r]-(m:Entity)\n",
    "                RETURN n.name AS source, type(r) AS rel, m.name AS target\n",
    "                \"\"\"\n",
    "                result = session.run(cypher, name=tok)\n",
    "                for record in result:\n",
    "                    src = record[\"source\"]\n",
    "                    rel = record[\"rel\"]\n",
    "                    tgt = record[\"target\"]\n",
    "                    triple_text = f\"{src} {rel} {tgt}.\"\n",
    "                    if triple_text not in seen_relations:\n",
    "                        seen_relations.add(triple_text)\n",
    "                        docs.append(Document(page_content=triple_text))\n",
    "\n",
    "        return docs\n",
    "\n",
    "    async def aget_relevant_documents(self, query: str) -> List[Document]:\n",
    "        # Pour la plupart des usages on peut renvoyer synchrone\n",
    "        return self.get_relevant_documents(query)\n",
    "\n",
    "    def __del__(self):\n",
    "        try:\n",
    "            self._driver.close()\n",
    "        except:\n",
    "            pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c187ca50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Chat GraphRAG (Neo4j + Albert via LangChain) ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\maadt\\AppData\\Local\\Temp\\ipykernel_4384\\4025754582.py:63: LangChainDeprecationWarning: The method `Chain.__call__` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  result = qa_chain({\"query\": question})\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìù R√©ponse :\n",
      "Je suis pr√™t √† r√©pondre √† votre question. Quelle est la question ?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Received notification from DBMS server: {severity: WARNING} {code: Neo.ClientNotification.Statement.UnknownLabelWarning} {category: UNRECOGNIZED} {title: The provided label is not in the database.} {description: One of the labels in your query is not available in the database, make sure you didn't misspell it or that the label is available when you run this statement in your application (the missing label name is: Entity)} {position: line: 2, column: 26, offset: 26} for query: '\\n                MATCH (n:Entity {name: $name})-[r]-(m:Entity)\\n                RETURN n.name AS source, type(r) AS rel, m.name AS target\\n                '\n",
      "Received notification from DBMS server: {severity: WARNING} {code: Neo.ClientNotification.Statement.UnknownLabelWarning} {category: UNRECOGNIZED} {title: The provided label is not in the database.} {description: One of the labels in your query is not available in the database, make sure you didn't misspell it or that the label is available when you run this statement in your application (the missing label name is: Entity)} {position: line: 2, column: 55, offset: 55} for query: '\\n                MATCH (n:Entity {name: $name})-[r]-(m:Entity)\\n                RETURN n.name AS source, type(r) AS rel, m.name AS target\\n                '\n",
      "Received notification from DBMS server: {severity: WARNING} {code: Neo.ClientNotification.Statement.UnknownLabelWarning} {category: UNRECOGNIZED} {title: The provided label is not in the database.} {description: One of the labels in your query is not available in the database, make sure you didn't misspell it or that the label is available when you run this statement in your application (the missing label name is: Entity)} {position: line: 2, column: 26, offset: 26} for query: '\\n                MATCH (n:Entity {name: $name})-[r]-(m:Entity)\\n                RETURN n.name AS source, type(r) AS rel, m.name AS target\\n                '\n",
      "Received notification from DBMS server: {severity: WARNING} {code: Neo.ClientNotification.Statement.UnknownLabelWarning} {category: UNRECOGNIZED} {title: The provided label is not in the database.} {description: One of the labels in your query is not available in the database, make sure you didn't misspell it or that the label is available when you run this statement in your application (the missing label name is: Entity)} {position: line: 2, column: 55, offset: 55} for query: '\\n                MATCH (n:Entity {name: $name})-[r]-(m:Entity)\\n                RETURN n.name AS source, type(r) AS rel, m.name AS target\\n                '\n",
      "Received notification from DBMS server: {severity: WARNING} {code: Neo.ClientNotification.Statement.UnknownLabelWarning} {category: UNRECOGNIZED} {title: The provided label is not in the database.} {description: One of the labels in your query is not available in the database, make sure you didn't misspell it or that the label is available when you run this statement in your application (the missing label name is: Entity)} {position: line: 2, column: 26, offset: 26} for query: '\\n                MATCH (n:Entity {name: $name})-[r]-(m:Entity)\\n                RETURN n.name AS source, type(r) AS rel, m.name AS target\\n                '\n",
      "Received notification from DBMS server: {severity: WARNING} {code: Neo.ClientNotification.Statement.UnknownLabelWarning} {category: UNRECOGNIZED} {title: The provided label is not in the database.} {description: One of the labels in your query is not available in the database, make sure you didn't misspell it or that the label is available when you run this statement in your application (the missing label name is: Entity)} {position: line: 2, column: 55, offset: 55} for query: '\\n                MATCH (n:Entity {name: $name})-[r]-(m:Entity)\\n                RETURN n.name AS source, type(r) AS rel, m.name AS target\\n                '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìù R√©ponse :\n",
      "D√©sol√©, je n'ai pas cette information.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Received notification from DBMS server: {severity: WARNING} {code: Neo.ClientNotification.Statement.UnknownLabelWarning} {category: UNRECOGNIZED} {title: The provided label is not in the database.} {description: One of the labels in your query is not available in the database, make sure you didn't misspell it or that the label is available when you run this statement in your application (the missing label name is: Entity)} {position: line: 2, column: 26, offset: 26} for query: '\\n                MATCH (n:Entity {name: $name})-[r]-(m:Entity)\\n                RETURN n.name AS source, type(r) AS rel, m.name AS target\\n                '\n",
      "Received notification from DBMS server: {severity: WARNING} {code: Neo.ClientNotification.Statement.UnknownLabelWarning} {category: UNRECOGNIZED} {title: The provided label is not in the database.} {description: One of the labels in your query is not available in the database, make sure you didn't misspell it or that the label is available when you run this statement in your application (the missing label name is: Entity)} {position: line: 2, column: 55, offset: 55} for query: '\\n                MATCH (n:Entity {name: $name})-[r]-(m:Entity)\\n                RETURN n.name AS source, type(r) AS rel, m.name AS target\\n                '\n",
      "Received notification from DBMS server: {severity: WARNING} {code: Neo.ClientNotification.Statement.UnknownLabelWarning} {category: UNRECOGNIZED} {title: The provided label is not in the database.} {description: One of the labels in your query is not available in the database, make sure you didn't misspell it or that the label is available when you run this statement in your application (the missing label name is: Entity)} {position: line: 2, column: 26, offset: 26} for query: '\\n                MATCH (n:Entity {name: $name})-[r]-(m:Entity)\\n                RETURN n.name AS source, type(r) AS rel, m.name AS target\\n                '\n",
      "Received notification from DBMS server: {severity: WARNING} {code: Neo.ClientNotification.Statement.UnknownLabelWarning} {category: UNRECOGNIZED} {title: The provided label is not in the database.} {description: One of the labels in your query is not available in the database, make sure you didn't misspell it or that the label is available when you run this statement in your application (the missing label name is: Entity)} {position: line: 2, column: 55, offset: 55} for query: '\\n                MATCH (n:Entity {name: $name})-[r]-(m:Entity)\\n                RETURN n.name AS source, type(r) AS rel, m.name AS target\\n                '\n",
      "Received notification from DBMS server: {severity: WARNING} {code: Neo.ClientNotification.Statement.UnknownLabelWarning} {category: UNRECOGNIZED} {title: The provided label is not in the database.} {description: One of the labels in your query is not available in the database, make sure you didn't misspell it or that the label is available when you run this statement in your application (the missing label name is: Entity)} {position: line: 2, column: 26, offset: 26} for query: '\\n                MATCH (n:Entity {name: $name})-[r]-(m:Entity)\\n                RETURN n.name AS source, type(r) AS rel, m.name AS target\\n                '\n",
      "Received notification from DBMS server: {severity: WARNING} {code: Neo.ClientNotification.Statement.UnknownLabelWarning} {category: UNRECOGNIZED} {title: The provided label is not in the database.} {description: One of the labels in your query is not available in the database, make sure you didn't misspell it or that the label is available when you run this statement in your application (the missing label name is: Entity)} {position: line: 2, column: 55, offset: 55} for query: '\\n                MATCH (n:Entity {name: $name})-[r]-(m:Entity)\\n                RETURN n.name AS source, type(r) AS rel, m.name AS target\\n                '\n",
      "Received notification from DBMS server: {severity: WARNING} {code: Neo.ClientNotification.Statement.UnknownLabelWarning} {category: UNRECOGNIZED} {title: The provided label is not in the database.} {description: One of the labels in your query is not available in the database, make sure you didn't misspell it or that the label is available when you run this statement in your application (the missing label name is: Entity)} {position: line: 2, column: 26, offset: 26} for query: '\\n                MATCH (n:Entity {name: $name})-[r]-(m:Entity)\\n                RETURN n.name AS source, type(r) AS rel, m.name AS target\\n                '\n",
      "Received notification from DBMS server: {severity: WARNING} {code: Neo.ClientNotification.Statement.UnknownLabelWarning} {category: UNRECOGNIZED} {title: The provided label is not in the database.} {description: One of the labels in your query is not available in the database, make sure you didn't misspell it or that the label is available when you run this statement in your application (the missing label name is: Entity)} {position: line: 2, column: 55, offset: 55} for query: '\\n                MATCH (n:Entity {name: $name})-[r]-(m:Entity)\\n                RETURN n.name AS source, type(r) AS rel, m.name AS target\\n                '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìù R√©ponse :\n",
      "Je connais la MAcif. La MAcif est un projet de v√©hicule autonome d√©velopp√© par le groupe PSA (Peugeot-Citro√´n) en collaboration avec l'Institut national de recherche en informatique et en automatique (INRIA) et l'Universit√© de Nice Sophia Antipolis. Le but de ce projet est de d√©velopper un v√©hicule autonome capable de conduire de mani√®re s√©curitaire et efficace dans des conditions r√©elles.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Received notification from DBMS server: {severity: WARNING} {code: Neo.ClientNotification.Statement.UnknownLabelWarning} {category: UNRECOGNIZED} {title: The provided label is not in the database.} {description: One of the labels in your query is not available in the database, make sure you didn't misspell it or that the label is available when you run this statement in your application (the missing label name is: Entity)} {position: line: 2, column: 26, offset: 26} for query: '\\n                MATCH (n:Entity {name: $name})-[r]-(m:Entity)\\n                RETURN n.name AS source, type(r) AS rel, m.name AS target\\n                '\n",
      "Received notification from DBMS server: {severity: WARNING} {code: Neo.ClientNotification.Statement.UnknownLabelWarning} {category: UNRECOGNIZED} {title: The provided label is not in the database.} {description: One of the labels in your query is not available in the database, make sure you didn't misspell it or that the label is available when you run this statement in your application (the missing label name is: Entity)} {position: line: 2, column: 55, offset: 55} for query: '\\n                MATCH (n:Entity {name: $name})-[r]-(m:Entity)\\n                RETURN n.name AS source, type(r) AS rel, m.name AS target\\n                '\n",
      "Received notification from DBMS server: {severity: WARNING} {code: Neo.ClientNotification.Statement.UnknownLabelWarning} {category: UNRECOGNIZED} {title: The provided label is not in the database.} {description: One of the labels in your query is not available in the database, make sure you didn't misspell it or that the label is available when you run this statement in your application (the missing label name is: Entity)} {position: line: 2, column: 26, offset: 26} for query: '\\n                MATCH (n:Entity {name: $name})-[r]-(m:Entity)\\n                RETURN n.name AS source, type(r) AS rel, m.name AS target\\n                '\n",
      "Received notification from DBMS server: {severity: WARNING} {code: Neo.ClientNotification.Statement.UnknownLabelWarning} {category: UNRECOGNIZED} {title: The provided label is not in the database.} {description: One of the labels in your query is not available in the database, make sure you didn't misspell it or that the label is available when you run this statement in your application (the missing label name is: Entity)} {position: line: 2, column: 55, offset: 55} for query: '\\n                MATCH (n:Entity {name: $name})-[r]-(m:Entity)\\n                RETURN n.name AS source, type(r) AS rel, m.name AS target\\n                '\n",
      "Received notification from DBMS server: {severity: WARNING} {code: Neo.ClientNotification.Statement.UnknownLabelWarning} {category: UNRECOGNIZED} {title: The provided label is not in the database.} {description: One of the labels in your query is not available in the database, make sure you didn't misspell it or that the label is available when you run this statement in your application (the missing label name is: Entity)} {position: line: 2, column: 26, offset: 26} for query: '\\n                MATCH (n:Entity {name: $name})-[r]-(m:Entity)\\n                RETURN n.name AS source, type(r) AS rel, m.name AS target\\n                '\n",
      "Received notification from DBMS server: {severity: WARNING} {code: Neo.ClientNotification.Statement.UnknownLabelWarning} {category: UNRECOGNIZED} {title: The provided label is not in the database.} {description: One of the labels in your query is not available in the database, make sure you didn't misspell it or that the label is available when you run this statement in your application (the missing label name is: Entity)} {position: line: 2, column: 55, offset: 55} for query: '\\n                MATCH (n:Entity {name: $name})-[r]-(m:Entity)\\n                RETURN n.name AS source, type(r) AS rel, m.name AS target\\n                '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìù R√©ponse :\n",
      "D√©sol√©, je n'ai pas cette information.\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.indexes.vectorstore import RetrievalQA\n",
    "# ----------------------------------------\n",
    "# 1. Charger les variables d‚Äôenvironnement\n",
    "# ----------------------------------------\n",
    "load_dotenv()\n",
    "\n",
    "# ----------------------------------------\n",
    "# 2. Instancier le LLM Albert\n",
    "# ----------------------------------------\n",
    "llm = AlbertLLM(temperature=0.2)\n",
    "\n",
    "# ----------------------------------------\n",
    "# 3. Cr√©er le prompt template pour la QA\n",
    "# ----------------------------------------\n",
    "# {context} = textes du graphe Neo4j\n",
    "# {question} = question utilisateur\n",
    "prompt_template = \"\"\"\n",
    "Tu es un assistant expert en v√©hicules autonomes.\n",
    "Voici le contexte extrait d'un graphe de connaissances :\n",
    "{context}\n",
    "\n",
    "Question : {question}\n",
    "\n",
    "R√©ponds de fa√ßon pr√©cise, en t‚Äôappuyant seulement sur ces faits. Si ce n‚Äôest pas dans le contexte, r√©pond ¬´ D√©sol√©, je n‚Äôai pas cette information. ¬ª.\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=prompt_template,\n",
    "    input_variables=[\"context\", \"question\"]\n",
    ")\n",
    "\n",
    "# ----------------------------------------\n",
    "# 4. Instancier le retriever Neo4j personnalis√©\n",
    "# ----------------------------------------\n",
    "graph_retriever = Neo4jRetriever()\n",
    "\n",
    "# ----------------------------------------\n",
    "# 5. Construire la cha√Æne RetrievalQA\n",
    "# ----------------------------------------\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",           # on bourre tout le contexte d‚Äôun coup\n",
    "    retriever=graph_retriever,\n",
    "    return_source_documents=False,\n",
    "    chain_type_kwargs={\"prompt\": prompt}\n",
    ")\n",
    "\n",
    "# ----------------------------------------\n",
    "# 6. Boucle interactive\n",
    "# ----------------------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"=== Chat GraphRAG (Neo4j + Albert via LangChain) ===\")\n",
    "    while True:\n",
    "        question = input(\"\\nPose ta question (ou ¬´ exit ¬ª pour quitter) : \")\n",
    "        if question.lower().strip() in (\"exit\", \"quit\"):\n",
    "            break\n",
    "\n",
    "        # LangChain :\n",
    "        #  1) graph_retriever.get_relevant_documents(question) ‚Üí liste de Docs\n",
    "        #  2) Concat√®ne ‚Äúcontext‚Äù = sommaire des docs, plus ‚Äúquestion‚Äù dans le prompt\n",
    "        #  3) Envoie tout √† llm._call(prompt_final) ‚Üí Albert ‚Üí g√©n√©ration\n",
    "        result = qa_chain({\"query\": question})\n",
    "        print(\"\\nüìù R√©ponse :\")\n",
    "        print(result[\"result\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b350558",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# ----------------------------------------\n",
    "# 2. D√©finir une classe AlbertLLM qui impl√©mente LLM de llama-index\n",
    "# ----------------------------------------\n",
    "class AlbertLLM(LLM):\n",
    "    \"\"\"\n",
    "    Impl√©mentation d'un LLM compatible LlamaIndex, pointant vers Albert API.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, temperature: float = 0.2):\n",
    "        self.temperature = temperature\n",
    "\n",
    "    @property\n",
    "    def metadata(self) -> dict:\n",
    "        # Quelques infos g√©n√©riques ; on peut ajuster si besoin\n",
    "        return {\"model_name\": \"gpt-3.5-turbo\", \"max_tokens\": 2048}\n",
    "\n",
    "    def _call(self, prompt: str, **kwargs) -> str:\n",
    "        \"\"\"\n",
    "        Envoie le prompt √† l‚ÄôAPI Albert (endpoint OpenAI-compatible) et retourne la r√©ponse.\n",
    "        \"\"\"\n",
    "        url = \"https://albert.api.etalab.gouv.fr/v1/chat/completions\"\n",
    "        headers = {\n",
    "            \"Authorization\": f\"Bearer {albert_api_key}\",\n",
    "            \"Content-Type\": \"application/json\"\n",
    "        }\n",
    "        json_data = {\n",
    "            \"model\": \"gpt-3.5-turbo\",\n",
    "            \"messages\": [{\"role\": \"user\", \"content\": prompt}],\n",
    "            \"temperature\": self.temperature,\n",
    "            # Vous pouvez ajouter \"max_tokens\": 1024, etc., si besoin\n",
    "        }\n",
    "        resp = requests.post(url, headers=headers, json=json_data)\n",
    "        resp.raise_for_status()\n",
    "        data = resp.json()\n",
    "        return data[\"choices\"][0][\"message\"][\"content\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89f54c16",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'BaseLLMPredictor' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[64]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# ----------------------------------------\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m# 3. D√©finir un ¬´ Predictor ¬ª qui impl√©mente BaseLLMPredictor\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# ----------------------------------------\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mAlbertLLMPredictor\u001b[39;00m(\u001b[43mBaseLLMPredictor\u001b[49m):\n\u001b[32m      5\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m      6\u001b[39m \u001b[33;03m    Wrapper autour d'AlbertLLM pour respecter l'interface BaseLLMPredictor\u001b[39;00m\n\u001b[32m      7\u001b[39m \u001b[33;03m    (v0.12 de llama-index).\u001b[39;00m\n\u001b[32m      8\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m     10\u001b[39m     \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, llm: AlbertLLM):\n",
      "\u001b[31mNameError\u001b[39m: name 'BaseLLMPredictor' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "# ----------------------------------------\n",
    "# 3. D√©finir un ¬´ Predictor ¬ª qui impl√©mente BaseLLMPredictor\n",
    "# ----------------------------------------\n",
    "class AlbertLLMPredictor(BaseLLMPredictor):\n",
    "    \"\"\"\n",
    "    Wrapper autour d'AlbertLLM pour respecter l'interface BaseLLMPredictor\n",
    "    (v0.12 de llama-index).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, llm: AlbertLLM):\n",
    "        self._llm = llm\n",
    "        self._callback_manager = CallbackManager()\n",
    "\n",
    "    @property\n",
    "    def llm(self) -> LLM:\n",
    "        return self._llm\n",
    "\n",
    "    @property\n",
    "    def callback_manager(self) -> CallbackManager:\n",
    "        return self._callback_manager\n",
    "\n",
    "    @property\n",
    "    def metadata(self) -> LLMMetadata:\n",
    "        return self._llm.metadata\n",
    "\n",
    "    def predict(self, prompt: BasePromptTemplate, **prompt_args) -> str:\n",
    "        \"\"\"\n",
    "        Construit une cha√Æne de caract√®res √† partir du BasePromptTemplate + args,\n",
    "        puis appelle le LLM pour obtenir la r√©ponse compl√®te.\n",
    "        \"\"\"\n",
    "        # prompt.format(prompt_args) renvoie la cha√Æne textuelle finale\n",
    "        text = prompt.format(**prompt_args)\n",
    "        return self._llm._call(text)\n",
    "\n",
    "    def stream(self, prompt: BasePromptTemplate, **prompt_args) -> TokenGen:\n",
    "        \"\"\"\n",
    "        Streaming non impl√©ment√© (renvoie tout en une fois).\n",
    "        \"\"\"\n",
    "        text = prompt.format(**prompt_args)\n",
    "        # Pour un vrai streaming, il faudrait appeler l‚ÄôAPI Albert en mode stream\n",
    "        # et yield() chaque token, mais ici on renvoie tout d‚Äôun coup :\n",
    "        yield self._llm._call(text)\n",
    "\n",
    "    async def apredict(self, prompt: BasePromptTemplate, **prompt_args) -> str:\n",
    "        \"\"\"\n",
    "        Async predict non impl√©ment√© : appelle _call synchronously pour l‚Äôinstant.\n",
    "        \"\"\"\n",
    "        text = prompt.format(**prompt_args)\n",
    "        return self._llm._call(text)\n",
    "\n",
    "    async def astream(self, prompt: BasePromptTemplate, **prompt_args) -> TokenAsyncGen:\n",
    "        \"\"\"\n",
    "        Async streaming non impl√©ment√© : renvoie tout d‚Äôun coup.\n",
    "        \"\"\"\n",
    "        text = prompt.format(**prompt_args)\n",
    "        yield self._llm._call(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e604f28",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Can't instantiate abstract class AlbertLLM without an implementation for abstract methods 'achat', 'acomplete', 'astream_chat', 'astream_complete', 'chat', 'complete', 'stream_chat', 'stream_complete'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[63]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# ----------------------------------------\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m# 3. Configurer le LLM Predictor avec AlbertLLM\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# ----------------------------------------\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m llm = \u001b[43mAlbertLLM\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.2\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      5\u001b[39m prompt_helper = PromptHelper()\n\u001b[32m      7\u001b[39m service_context = ServiceContext.from_defaults(\n\u001b[32m      8\u001b[39m     llm=llm,\n\u001b[32m      9\u001b[39m     prompt_helper=prompt_helper\n\u001b[32m     10\u001b[39m )\n",
      "\u001b[31mTypeError\u001b[39m: Can't instantiate abstract class AlbertLLM without an implementation for abstract methods 'achat', 'acomplete', 'astream_chat', 'astream_complete', 'chat', 'complete', 'stream_chat', 'stream_complete'"
     ]
    }
   ],
   "source": [
    "# ----------------------------------------\n",
    "# 3. Configurer le LLM Predictor avec AlbertLLM\n",
    "# ----------------------------------------\n",
    "llm = AlbertLLM(temperature=0.2)\n",
    "prompt_helper = PromptHelper()\n",
    "\n",
    "service_context = ServiceContext.from_defaults(\n",
    "    llm=llm,\n",
    "    prompt_helper=prompt_helper\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06e878a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------\n",
    "# 4. Construire l‚Äôindex GraphRAG avec LlamaIndex\n",
    "# ----------------------------------------\n",
    "graph_index = KnowledgeGraphIndex.from_dicts(\n",
    "    all_triples,\n",
    "    service_context=service_context\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b8c5928",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------\n",
    "# 5. Fonction de r√©ponse via GraphRAG + Albert\n",
    "# ----------------------------------------\n",
    "def answer_from_graph_index(question: str) -> str:\n",
    "    \"\"\"\n",
    "    Interroge le KnowledgeGraphIndex. LlamaIndex parcourt le graphe interne\n",
    "    et g√©n√®re la r√©ponse en appelant AlbertLLM pour la compl√©tion finale.\n",
    "    \"\"\"\n",
    "    response = graph_index.query(question)\n",
    "    return response.response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daa0ee4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------\n",
    "# 6. Exemple d‚Äôutilisation\n",
    "# ----------------------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    q = \"Que vise la MACIF dans le domaine du v√©hicule autonome ?\"\n",
    "    answer = answer_from_graph_index(q)\n",
    "    print(\"Question :\", q)\n",
    "    print(\"R√©ponse :\", answer)\n",
    "\n",
    "    driver.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73da0326",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# 1. Transformer les triples en texte\n",
    "triples_text = \"\\n\".join(\n",
    "    f\"{t['source']} {t['relation']} {t['target']}.\" for t in all_triples\n",
    ")\n",
    "\n",
    "# 2. Cr√©er un document\n",
    "documents = [Document(triples_text)]\n",
    "\n",
    "# 3. Parser les documents\n",
    "node_parser = SimpleNodeParser()\n",
    "nodes = node_parser.get_nodes_from_documents(documents)\n",
    "\n",
    "# 4. Cr√©er un graph store en m√©moire\n",
    "graph_store = SimpleGraphStore()\n",
    "\n",
    "# 5. Construire l‚Äôindex KnowledgeGraphIndex\n",
    "service_context = ServiceContext.from_defaults(llm=OpenAI(temperature=0))\n",
    "kg_index = KnowledgeGraphIndex(\n",
    "    nodes=nodes,\n",
    "    graph_store=graph_store,\n",
    "    service_context=service_context\n",
    ")\n",
    "\n",
    "# 6. Interroger l‚Äôindex\n",
    "query = \"Quels capteurs sont utilis√©s par Tesla ?\"\n",
    "response = kg_index.query(query)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e2f0387",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "type object 'KnowledgeGraphIndex' has no attribute 'from_dicts'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# ----------------------------------------\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m# 4. Construire l‚Äôindex GraphRAG avec LlamaIndex\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# ----------------------------------------\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# Ici, on cr√©e un KnowledgeGraphIndex directement √† partir de la liste de dicts.\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m graph_index = \u001b[43mKnowledgeGraphIndex\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_dicts\u001b[49m(\n\u001b[32m      6\u001b[39m     all_triples,\n\u001b[32m      7\u001b[39m     service_context=service_context\n\u001b[32m      8\u001b[39m )\n",
      "\u001b[31mAttributeError\u001b[39m: type object 'KnowledgeGraphIndex' has no attribute 'from_dicts'"
     ]
    }
   ],
   "source": [
    "# ----------------------------------------\n",
    "# 4. Construire l‚Äôindex GraphRAG avec LlamaIndex\n",
    "# ----------------------------------------\n",
    "# Ici, on cr√©e un KnowledgeGraphIndex directement √† partir de la liste de dicts.\n",
    "graph_index = KnowledgeGraphIndex.from_dicts(\n",
    "    all_triples,\n",
    "    service_context=service_context\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e5840ec",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'llama_index.indices.knowledge_graph'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m      5\u001b[39m     \u001b[38;5;66;03m# Pour LlamaIndex 0.8.x\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mllama_index\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mindices\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mknowledge_graph\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mbase\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m KGTable, KnowledgeGraphIndex\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n\u001b[32m      8\u001b[39m     \u001b[38;5;66;03m# Pour LlamaIndex 0.9+ (chemin possible)\u001b[39;00m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'llama_index.indices.knowledge_graph'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 9\u001b[39m\n\u001b[32m      6\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mllama_index\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mindices\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mknowledge_graph\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mbase\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m KGTable, KnowledgeGraphIndex\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n\u001b[32m      8\u001b[39m     \u001b[38;5;66;03m# Pour LlamaIndex 0.9+ (chemin possible)\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mllama_index\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mindices\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mknowledge_graph\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mschema\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m KGTable\n\u001b[32m     10\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mllama_index\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mindices\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mknowledge_graph\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mbase\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m KnowledgeGraphIndex\n\u001b[32m     12\u001b[39m \u001b[38;5;66;03m# 2) Construire la table √† partir d'une liste de dicts\u001b[39;00m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'llama_index.indices.knowledge_graph'"
     ]
    }
   ],
   "source": [
    "# import llama_index\n",
    "# # llamaindex_demo.py (suite)\n",
    "# # 1) Import du KGTable et KnowledgeGraphIndex\n",
    "# try:\n",
    "#     # Pour LlamaIndex 0.8.x\n",
    "#     from llama_index.indices.knowledge_graph.base import KGTable, KnowledgeGraphIndex\n",
    "# except ImportError:\n",
    "#     # Pour LlamaIndex 0.9+ (chemin possible)\n",
    "#     from llama_index.indices.knowledge_graph.schema import KGTable\n",
    "#     from llama_index.indices.knowledge_graph.base import KnowledgeGraphIndex\n",
    "\n",
    "# # 2) Construire la table √† partir d'une liste de dicts\n",
    "# kg_table = KGTable.from_list(all_triples)\n",
    "\n",
    "# # 3) Reprendre le service_context d√©j√† configur√© plus haut\n",
    "# #    (llm_predictor + prompt_helper)\n",
    "\n",
    "# # 4) Cr√©er l'index GraphRAG √† partir du KGTable\n",
    "# graph_index = KnowledgeGraphIndex.from_kg_table(\n",
    "#     kg_table,\n",
    "#     service_context=service_context\n",
    "# )\n",
    "\n",
    "# # 5) Exemple de requ√™te GraphRAG\n",
    "# def answer_from_graph_index(question: str) -> str:\n",
    "#     response = graph_index.query(question)\n",
    "#     return response.response\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     q = \"Que vise la MACIF dans le domaine du v√©hicule autonome ?\"\n",
    "#     print(answer_from_graph_index(q))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
