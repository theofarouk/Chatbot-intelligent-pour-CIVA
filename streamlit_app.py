import streamlit as st
from langchain.prompts import PromptTemplate
from langchain.indexes.vectorstore import RetrievalQA
from dotenv import load_dotenv
from retriever import MistralLangChainLLM  # remplace par ta classe
from Retriever import Neo4jRetriever


# ----------------------------------------
# 1. Charger les variables dâ€™environnement
# ----------------------------------------
load_dotenv()

# ----------------------------------------
# 2. Instancier le LLM Albert
# ----------------------------------------
llm = MistralLangChainLLM(temperature=0.2)

# ----------------------------------------
# 3. CrÃ©er le prompt template pour la QA
# ----------------------------------------
# {context} = textes du graphe Neo4j
# {question} = question utilisateur
prompt_template = """
Tu es un assistant expert en vÃ©hicules autonomes.
Voici le contexte extrait d'un graphe de connaissances :
{context}

Question : {question}

RÃ©ponds de faÃ§on prÃ©cise, en tâ€™appuyant seulement sur ces faits. Si ce nâ€™est pas dans le contexte, rÃ©pond Â« DÃ©solÃ©, je nâ€™ai pas cette information. Â».
"""

prompt = PromptTemplate(
    template=prompt_template,
    input_variables=["context", "question"]
)

# ----------------------------------------
# 4. Instancier le retriever Neo4j personnalisÃ©
# ----------------------------------------
graph_retriever = Neo4jRetriever()

# ----------------------------------------
# 5. Construire la chaÃ®ne RetrievalQA
# ----------------------------------------
qa_chain = RetrievalQA.from_chain_type(
    llm=llm,
    chain_type="stuff",           # on bourre tout le contexte dâ€™un coup
    retriever=graph_retriever,
    return_source_documents=False,
    chain_type_kwargs={"prompt": prompt}
)

# ----------------------------------------
# 6. Boucle interactive
# ----------------------------------------
if __name__ == "__main__":
    print("=== Chat GraphRAG (Neo4j + Albert via LangChain) ===")
    while True:
        question = input("\nPose ta question (ou Â« exit Â» pour quitter) : ")
        if question.lower().strip() in ("exit", "quit"):
            break

        # LangChain :
        #  1) graph_retriever.get_relevant_documents(question) â†’ liste de Docs
        #  2) ConcatÃ¨ne â€œcontextâ€ = sommaire des docs, plus â€œquestionâ€ dans le prompt
        #  3) Envoie tout Ã  llm._call(prompt_final) â†’ Albert â†’ gÃ©nÃ©ration
        result = qa_chain.invoke({"query": question})
        print("\nğŸ“ RÃ©ponse :")
        print(result["result"])