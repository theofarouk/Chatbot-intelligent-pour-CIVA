{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a28fba69",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(\"outputs/all_triples.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    all_triples = json.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0ac3eef0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\maadt\\AppData\\Local\\Temp\\ipykernel_3636\\2666090052.py:41: DeprecationWarning: write_transaction has been renamed to execute_write\n",
      "  session.write_transaction(insert_triplet, src, rel, tgt)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 26746 triplets insérés dans Neo4j.\n"
     ]
    }
   ],
   "source": [
    "from neo4j import GraphDatabase\n",
    "\n",
    "# ---------- 1. Configuration de la connexion Neo4j ----------\n",
    "# Remplacez ces valeurs par celles de votre instance Neo4j\n",
    "uri  = \"bolt://localhost:7687\"\n",
    "user = \"neo4j\"\n",
    "pwd  = \"BNEOsucks8921_\"\n",
    "\n",
    "# Crée un driver Neo4j\n",
    "driver = GraphDatabase.driver(uri, auth=(user, pwd))\n",
    "\n",
    "\n",
    "# ---------- 2. Fonction d’insertion d’un triplet dans Neo4j ----------\n",
    "def insert_triplet(tx, source, relation, target):\n",
    "    \"\"\"\n",
    "    Crée ou récupère deux nœuds (Entity) nommés 'source' et 'target', \n",
    "    puis crée (si nécessaire) la relation entre eux.\n",
    "    Le type de relation est stocké dans la propriété 'type' de l'arc.\n",
    "\n",
    "    Exemple Cypher généré :\n",
    "      MERGE (a:Entity {name: $source})\n",
    "      MERGE (b:Entity {name: $target})\n",
    "      MERGE (a)-[:RELATION {type: $relation}]->(b)\n",
    "    \"\"\"\n",
    "    query = \"\"\"\n",
    "    MERGE (a:Source {name: $source})\n",
    "    MERGE (b:Target {name: $target})\n",
    "    MERGE (a)-[:RELATION {type: $relation}]->(b)\n",
    "    \"\"\"\n",
    "    tx.run(query, source=source, relation=relation, target=target)\n",
    "\n",
    "\n",
    "# ---------- 3. Fonction de chargement de tous les triplets ----------\n",
    "def load_all_triplets(triplets):\n",
    "    \"\"\"\n",
    "    Parcourt la liste 'triplets' (liste de tuples (source, relation, target))\n",
    "    et exécute 'insert_triplet' pour chacun d’entre eux dans une même session.\n",
    "    \"\"\"\n",
    "    with driver.session() as session:\n",
    "        for src, rel, tgt in triplets:\n",
    "            session.write_transaction(insert_triplet, src, rel, tgt)\n",
    "    print(f\"✅ {len(triplets)} triplets insérés dans Neo4j.\")\n",
    "\n",
    "\n",
    "# ---------- 4. Point d’entrée du script ----------\n",
    "if __name__ == \"__main__\":\n",
    "    # Supposons que vous ayez, dans un autre module, votre liste 'all_triples'\n",
    "    # Exemple : all_triples = [(\"Navette autonome\", \"testée dans\", \"Toulouse\"), ...]\n",
    "    # Il faut donc importer ou recréer cette liste ici.\n",
    "    #\n",
    "    # Si votre extraction a produit un module Python ou un fichier pickle,\n",
    "    # récupérez la liste de tuples et assignez-la à 'all_triples'.\n",
    "    #\n",
    "    # Par exemple, si vous avez sauvegardé vos relations dans un fichier JSON :\n",
    "    #\n",
    "    # import json\n",
    "    # with open(\"outputs/all_triples.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    #     all_triples = json.load(f)\n",
    "    #\n",
    "    # Mais ici, illustrons un exemple statique (à remplacer par votre propre liste) :\n",
    "\n",
    "    # Charge tous les triplets dans Neo4j\n",
    "    load_all_triplets(all_triples)\n",
    "\n",
    "    # Ferme le driver une fois terminé\n",
    "    driver.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f7d82ba6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\maadt\\AppData\\Local\\Temp\\ipykernel_3636\\1368597255.py:10: DeprecationWarning: Using a driver after it has been closed is deprecated. Future versions of the driver will raise an error.\n",
      "  with driver.session() as session:\n"
     ]
    }
   ],
   "source": [
    "def fetch_all_triples():\n",
    "    \"\"\"\n",
    "    Récupère l'ensemble des triplets (entité, relation, entité) dans Neo4j.\n",
    "    \"\"\"\n",
    "    query = \"\"\"\n",
    "    MATCH (a:Source)-[r:RELATION]->(b:Target)\n",
    "    RETURN a.name AS source, r.type AS relation, b.name AS target\n",
    "    \"\"\"\n",
    "    triples = []\n",
    "    with driver.session() as session:\n",
    "        for record in session.run(query):\n",
    "            triples.append({\n",
    "                \"source\": record[\"source\"],\n",
    "                \"relation\": record[\"relation\"],\n",
    "                \"target\": record[\"target\"]\n",
    "            })\n",
    "    return triples\n",
    "\n",
    "# Exemple :\n",
    "all_triples = fetch_all_triples()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "93db417a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.service_context_elements.llm_predictor import LLMPredictor\n",
    "from llama_index.core import PromptHelper, ServiceContext\n",
    "from llama_index.cli.rag.base import LLM\n",
    "from llama_index.core.indices import KnowledgeGraphIndex\n",
    "#from llama_index.indices.knowledge_graph.schema import KGTable\n",
    "from llama_index.graph_stores.neo4j import Neo4jPropertyGraphStore\n",
    "from dotenv import load_dotenv\n",
    "from llama_index.core.indices import PropertyGraphIndex\n",
    "import os\n",
    "import requests\n",
    "from neo4j import GraphDatabase\n",
    "import openai\n",
    "from langchain.llms.base import LLM\n",
    "from typing import Optional, List, Mapping, Any\n",
    "from pydantic import BaseModel\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7c6a55d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e24472b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mistralai.client import MistralClient\n",
    "from langchain_core.language_models import LLM\n",
    "from typing import Optional, List, Mapping, Any\n",
    "from pydantic import BaseModel, PrivateAttr\n",
    "import os\n",
    "\n",
    "class MistralLangChainLLM(LLM, BaseModel):\n",
    "    \"\"\"\n",
    "    Wrapper LangChain-compatible pour le SDK mistralai.\n",
    "    \"\"\"\n",
    "\n",
    "    temperature: float = 0.2\n",
    "    model_name: str = \"mistral-small\"\n",
    "    \n",
    "    # ✅ Ajout d'attributs privés compatibles Pydantic\n",
    "    _api_key: str = PrivateAttr()\n",
    "    _client: MistralClient = PrivateAttr()\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self._api_key = os.getenv(\"MISTRAL_API_KEY\")\n",
    "        if not self._api_key:\n",
    "            raise ValueError(\"⚠️ La clé API MISTRAL_API_KEY est manquante dans l’environnement.\")\n",
    "        self._client = MistralClient(api_key=self._api_key)\n",
    "\n",
    "    def _call(self, prompt: str, stop: Optional[List[str]] = None) -> str:\n",
    "        messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "        response = self._client.chat(\n",
    "            model=self.model_name,\n",
    "            messages=messages,\n",
    "            temperature=self.temperature,\n",
    "        )\n",
    "        return response.choices[0].message.content\n",
    "\n",
    "    @property\n",
    "    def _llm_type(self) -> str:\n",
    "        return \"mistral\"\n",
    "\n",
    "    @property\n",
    "    def _identifying_params(self) -> Mapping[str, Any]:\n",
    "        return {\n",
    "            \"model_name\": self.model_name,\n",
    "            \"temperature\": self.temperature,\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a51495e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\maadt\\AppData\\Local\\Temp\\ipykernel_17420\\3120548857.py:8: DeprecationWarning: Retrievers must implement abstract `_get_relevant_documents` method instead of `get_relevant_documents`\n",
      "  class Neo4jRetriever(BaseRetriever):\n",
      "C:\\Users\\maadt\\AppData\\Local\\Temp\\ipykernel_17420\\3120548857.py:8: DeprecationWarning: Retrievers must implement abstract `_aget_relevant_documents` method instead of `aget_relevant_documents`\n",
      "  class Neo4jRetriever(BaseRetriever):\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "import os\n",
    "from typing import List, Any\n",
    "from neo4j import GraphDatabase\n",
    "from langchain.schema import BaseRetriever, Document\n",
    "from pydantic import PrivateAttr\n",
    "\n",
    "\n",
    "class Neo4jRetriever(BaseRetriever):\n",
    "    \"\"\"\n",
    "    Retriever LangChain qui interroge Neo4j pour ramener un sous-graphe pertinent.\n",
    "    \"\"\"\n",
    "\n",
    "    # Déclare driver comme attribut privé afin que Pydantic ne l'exige pas comme champ\n",
    "    _driver: Any = PrivateAttr()\n",
    "\n",
    "    def __init__(self):\n",
    "        # Appelle le constructeur de BaseModel\n",
    "        super().__init__()\n",
    "\n",
    "        uri = os.getenv(\"NEO4J_URI\")\n",
    "        user = os.getenv(\"NEO4J_USER\")\n",
    "        pwd  = os.getenv(\"NEO4J_PWD\")\n",
    "        self._driver = GraphDatabase.driver(uri, auth=(user, pwd))\n",
    "\n",
    "    def get_relevant_documents(self, query: str) -> List[Document]:\n",
    "        \"\"\"\n",
    "        1) On extrait des tokens (mots) de la question,\n",
    "        2) On interroge Neo4j pour chaque token correspondant\n",
    "           à un nœud Entity.name,\n",
    "        3) On construit un Document par relation trouvée.\n",
    "        \"\"\"\n",
    "        # Tokenisation basique ; dans la vraie vie, on ferait un NER ou des lowercase+strip\n",
    "        tokens = [tok.strip() for tok in query.split() if len(tok) > 1]\n",
    "        seen_relations = set()\n",
    "        docs: List[Document] = []\n",
    "\n",
    "        with self._driver.session() as session:\n",
    "            for tok in tokens:\n",
    "                cypher = \"\"\"\n",
    "                MATCH (n:Entity {name: $name})-[r]-(m:Entity)\n",
    "                RETURN n.name AS source, type(r) AS rel, m.name AS target\n",
    "                \"\"\"\n",
    "                result = session.run(cypher, name=tok)\n",
    "                for record in result:\n",
    "                    src = record[\"source\"]\n",
    "                    rel = record[\"rel\"]\n",
    "                    tgt = record[\"target\"]\n",
    "                    triple_text = f\"{src} {rel} {tgt}.\"\n",
    "                    if triple_text not in seen_relations:\n",
    "                        seen_relations.add(triple_text)\n",
    "                        docs.append(Document(page_content=triple_text))\n",
    "\n",
    "        return docs\n",
    "\n",
    "    async def aget_relevant_documents(self, query: str) -> List[Document]:\n",
    "        # Pour la plupart des usages on peut renvoyer synchrone\n",
    "        return self.get_relevant_documents(query)\n",
    "\n",
    "    def __del__(self):\n",
    "        try:\n",
    "            self._driver.close()\n",
    "        except:\n",
    "            pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d343912d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Chat GraphRAG (Neo4j + Albert via LangChain) ===\n",
      "\n",
      "📝 Réponse :\n",
      "L'automobile est liée à la mobilité, ce qui signifie qu'elle joue un rôle important dans le domaine des déplacements. Elle est également associée à la France, indiquant que la France a une relation avec l'industrie automobile, que ce soit en termes de production, de consommation ou de réglementation. En outre, l'automobile a également une relation avec la population, ce qui suggère que les voitures sont un moyen de transport important pour les gens. Cependant, ce contexte ne fournit pas d'informations spécifiques sur le fonctionnement, les caractéristiques ou les types d'automobiles.\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.indexes.vectorstore import RetrievalQA\n",
    "import dotenv\n",
    "from Retr\n",
    "\n",
    "# ----------------------------------------\n",
    "# 1. Charger les variables d’environnement\n",
    "# ----------------------------------------\n",
    "load_dotenv()\n",
    "\n",
    "# ----------------------------------------\n",
    "# 2. Instancier le LLM Albert\n",
    "# ----------------------------------------\n",
    "llm = MistralLangChainLLM(temperature=0.2)\n",
    "\n",
    "# ----------------------------------------\n",
    "# 3. Créer le prompt template pour la QA\n",
    "# ----------------------------------------\n",
    "# {context} = textes du graphe Neo4j\n",
    "# {question} = question utilisateur\n",
    "prompt_template = \"\"\"\n",
    "Tu es un assistant expert en véhicules autonomes.\n",
    "Voici le contexte extrait d'un graphe de connaissances :\n",
    "{context}\n",
    "\n",
    "Question : {question}\n",
    "\n",
    "Réponds de façon précise, en t’appuyant seulement sur ces faits. Si ce n’est pas dans le contexte, répond « Désolé, je n’ai pas cette information. ».\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=prompt_template,\n",
    "    input_variables=[\"context\", \"question\"]\n",
    ")\n",
    "\n",
    "# ----------------------------------------\n",
    "# 4. Instancier le retriever Neo4j personnalisé\n",
    "# ----------------------------------------\n",
    "graph_retriever = Neo4jRetriever()\n",
    "\n",
    "# ----------------------------------------\n",
    "# 5. Construire la chaîne RetrievalQA\n",
    "# ----------------------------------------\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",           # on bourre tout le contexte d’un coup\n",
    "    retriever=graph_retriever,\n",
    "    return_source_documents=False,\n",
    "    chain_type_kwargs={\"prompt\": prompt}\n",
    ")\n",
    "\n",
    "# ----------------------------------------\n",
    "# 6. Boucle interactive\n",
    "# ----------------------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"=== Chat GraphRAG (Neo4j + Albert via LangChain) ===\")\n",
    "    while True:\n",
    "        question = input(\"\\nPose ta question (ou « exit » pour quitter) : \")\n",
    "        if question.lower().strip() in (\"exit\", \"quit\"):\n",
    "            break\n",
    "\n",
    "        # LangChain :\n",
    "        #  1) graph_retriever.get_relevant_documents(question) → liste de Docs\n",
    "        #  2) Concatène “context” = sommaire des docs, plus “question” dans le prompt\n",
    "        #  3) Envoie tout à llm._call(prompt_final) → Albert → génération\n",
    "        result = qa_chain.invoke({\"query\": question})\n",
    "        print(\"\\n📝 Réponse :\")\n",
    "        print(result[\"result\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
