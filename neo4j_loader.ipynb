{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a28fba69",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(\"outputs/all_triples.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    all_triples = json.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0ac3eef0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\maadt\\AppData\\Local\\Temp\\ipykernel_37420\\651748714.py:41: DeprecationWarning: write_transaction has been renamed to execute_write\n",
      "  session.write_transaction(insert_triplet, src, rel, tgt)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 26746 triplets insérés dans Neo4j.\n"
     ]
    }
   ],
   "source": [
    "from neo4j import GraphDatabase\n",
    "\n",
    "# ---------- 1. Configuration de la connexion Neo4j ----------\n",
    "# Remplacez ces valeurs par celles de votre instance Neo4j\n",
    "uri  = \"bolt://localhost:7687\"\n",
    "user = \"neo4j\"\n",
    "pwd  = \"BNEOsucks8921_\"\n",
    "\n",
    "# Crée un driver Neo4j\n",
    "driver = GraphDatabase.driver(uri, auth=(user, pwd))\n",
    "\n",
    "\n",
    "# ---------- 2. Fonction d’insertion d’un triplet dans Neo4j ----------\n",
    "def insert_triplet(tx, source, relation, target):\n",
    "    \"\"\"\n",
    "    Crée ou récupère deux nœuds (Entity) nommés 'source' et 'target', \n",
    "    puis crée (si nécessaire) la relation entre eux.\n",
    "    Le type de relation est stocké dans la propriété 'type' de l'arc.\n",
    "\n",
    "    Exemple Cypher généré :\n",
    "      MERGE (a:Entity {name: $source})\n",
    "      MERGE (b:Entity {name: $target})\n",
    "      MERGE (a)-[:RELATION {type: $relation}]->(b)\n",
    "    \"\"\"\n",
    "    query = \"\"\"\n",
    "    MERGE (a:Entity {name: $source})\n",
    "    MERGE (b:Entity {name: $target})\n",
    "    MERGE (a)-[:RELATION {type: $relation}]->(b)\n",
    "    \"\"\"\n",
    "    tx.run(query, source=source, relation=relation, target=target)\n",
    "\n",
    "\n",
    "# ---------- 3. Fonction de chargement de tous les triplets ----------\n",
    "def load_all_triplets(triplets):\n",
    "    \"\"\"\n",
    "    Parcourt la liste 'triplets' (liste de tuples (source, relation, target))\n",
    "    et exécute 'insert_triplet' pour chacun d’entre eux dans une même session.\n",
    "    \"\"\"\n",
    "    with driver.session() as session:\n",
    "        for src, rel, tgt in triplets:\n",
    "            session.write_transaction(insert_triplet, src, rel, tgt)\n",
    "    print(f\"✅ {len(triplets)} triplets insérés dans Neo4j.\")\n",
    "\n",
    "\n",
    "# ---------- 4. Point d’entrée du script ----------\n",
    "if __name__ == \"__main__\":\n",
    "    # Supposons que vous ayez, dans un autre module, votre liste 'all_triples'\n",
    "    # Exemple : all_triples = [(\"Navette autonome\", \"testée dans\", \"Toulouse\"), ...]\n",
    "    # Il faut donc importer ou recréer cette liste ici.\n",
    "    #\n",
    "    # Si votre extraction a produit un module Python ou un fichier pickle,\n",
    "    # récupérez la liste de tuples et assignez-la à 'all_triples'.\n",
    "    #\n",
    "    # Par exemple, si vous avez sauvegardé vos relations dans un fichier JSON :\n",
    "    #\n",
    "    # import json\n",
    "    # with open(\"outputs/all_triples.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    #     all_triples = json.load(f)\n",
    "    #\n",
    "    # Mais ici, illustrons un exemple statique (à remplacer par votre propre liste) :\n",
    "\n",
    "    # Charge tous les triplets dans Neo4j\n",
    "    load_all_triplets(all_triples)\n",
    "\n",
    "    # Ferme le driver une fois terminé\n",
    "    driver.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7d82ba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_all_triples():\n",
    "    \"\"\"\n",
    "    Récupère l'ensemble des triplets (entité, relation, entité) dans Neo4j.\n",
    "    \"\"\"\n",
    "    query = \"\"\"\n",
    "    MATCH (a:Source)-[r:RELATION]->(b:Target)\n",
    "    RETURN a.name AS source, r.type AS relation, b.name AS target\n",
    "    \"\"\"\n",
    "    triples = []\n",
    "    with driver.session() as session:\n",
    "        for record in session.run(query):\n",
    "            triples.append({\n",
    "                \"source\": record[\"source\"],\n",
    "                \"relation\": record[\"relation\"],\n",
    "                \"target\": record[\"target\"]\n",
    "            })\n",
    "    return triples\n",
    "\n",
    "# Exemple :\n",
    "all_triples = fetch_all_triples()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "93db417a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.service_context_elements.llm_predictor import LLMPredictor\n",
    "from llama_index.core import PromptHelper, ServiceContext\n",
    "from llama_index.cli.rag.base import LLM\n",
    "from llama_index.core.indices import KnowledgeGraphIndex\n",
    "#from llama_index.indices.knowledge_graph.schema import KGTable\n",
    "from llama_index.graph_stores.neo4j import Neo4jPropertyGraphStore\n",
    "from dotenv import load_dotenv\n",
    "from llama_index.core.indices import PropertyGraphIndex\n",
    "import os\n",
    "import requests\n",
    "from neo4j import GraphDatabase\n",
    "import openai\n",
    "from langchain.llms.base import LLM\n",
    "from typing import Optional, List, Mapping, Any\n",
    "from pydantic import BaseModel\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "25cdc73b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) On configure openai pour pointer vers Albert\n",
    "openai.api_key = 'sk-eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJ1c2VyX2lkIjo4NDAyLCJ0b2tlbl9pZCI6MTQ4OSwiZXhwaXJlc19hdCI6MTc4MDM1MTIwMH0.mOB9Cx4U4G7K5gin0twePc_WauAEPtRWQ0UaK6oUs9I'\n",
    "openai.api_base = \"https://albert.api.etalab.gouv.fr/v1\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a3fc45e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AlbertLLM(LLM, BaseModel):\n",
    "    \"\"\"\n",
    "    Wrapper LangChain pour Albert (API OpenAI-compatible).\n",
    "    \"\"\"\n",
    "\n",
    "    temperature: float = 0.2\n",
    "    model_name: str = \"albert-small\"\n",
    "\n",
    "    class Config:\n",
    "        \"\"\"Pour que pydantic accepte les champs supplémentaires (ignorez-les).\"\"\"\n",
    "        extra = \"ignore\"\n",
    "\n",
    "    def _call(self, prompt: str, stop: Optional[List[str]] = None) -> str:\n",
    "        \"\"\"\n",
    "        Envoie le prompt à l’API Albert et renvoie le texte généré.\n",
    "        \"\"\"\n",
    "        response = openai.ChatCompletion.create(\n",
    "            model=self.model_name,\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            temperature=self.temperature,\n",
    "            stop=stop,\n",
    "            max_tokens=1024,\n",
    "        )\n",
    "        return response.choices[0].message.content\n",
    "\n",
    "    @property\n",
    "    def _identifying_params(self) -> Mapping[str, Any]:\n",
    "        return {\"model_name\": self.model_name, \"temperature\": self.temperature}\n",
    "\n",
    "    @property\n",
    "    def _llm_type(self) -> str:\n",
    "        return \"albert\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a51495e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\maadt\\AppData\\Local\\Temp\\ipykernel_39392\\3056014003.py:7: DeprecationWarning: Retrievers must implement abstract `_get_relevant_documents` method instead of `get_relevant_documents`\n",
      "  class Neo4jRetriever(BaseRetriever):\n",
      "C:\\Users\\maadt\\AppData\\Local\\Temp\\ipykernel_39392\\3056014003.py:7: DeprecationWarning: Retrievers must implement abstract `_aget_relevant_documents` method instead of `aget_relevant_documents`\n",
      "  class Neo4jRetriever(BaseRetriever):\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "import os\n",
    "from typing import List, Any\n",
    "from neo4j import GraphDatabase\n",
    "from langchain.schema import BaseRetriever, Document\n",
    "from pydantic import PrivateAttr\n",
    "\n",
    "\n",
    "class Neo4jRetriever(BaseRetriever):\n",
    "    \"\"\"\n",
    "    Retriever LangChain qui interroge Neo4j pour ramener un sous-graphe pertinent.\n",
    "    \"\"\"\n",
    "\n",
    "    # Déclare driver comme attribut privé afin que Pydantic ne l'exige pas comme champ\n",
    "    _driver: Any = PrivateAttr()\n",
    "\n",
    "    def __init__(self):\n",
    "        # Appelle le constructeur de BaseModel\n",
    "        super().__init__()\n",
    "\n",
    "        uri = os.getenv(\"NEO4J_URI\")\n",
    "        user = os.getenv(\"NEO4J_USER\")\n",
    "        pwd  = os.getenv(\"NEO4J_PWD\")\n",
    "        self._driver = GraphDatabase.driver(uri, auth=(user, pwd))\n",
    "\n",
    "    def get_relevant_documents(self, query: str) -> List[Document]:\n",
    "        \"\"\"\n",
    "        1) On extrait des tokens (mots) de la question,\n",
    "        2) On interroge Neo4j pour chaque token correspondant\n",
    "           à un nœud Entity.name,\n",
    "        3) On construit un Document par relation trouvée.\n",
    "        \"\"\"\n",
    "        # Tokenisation basique ; dans la vraie vie, on ferait un NER ou des lowercase+strip\n",
    "        tokens = [tok.strip() for tok in query.split() if len(tok) > 1]\n",
    "        seen_relations = set()\n",
    "        docs: List[Document] = []\n",
    "\n",
    "        with self._driver.session() as session:\n",
    "            for tok in tokens:\n",
    "                cypher = \"\"\"\n",
    "                MATCH (n:Entity {name: $name})-[r]-(m:Entity)\n",
    "                RETURN n.name AS source, type(r) AS rel, m.name AS target\n",
    "                \"\"\"\n",
    "                result = session.run(cypher, name=tok)\n",
    "                for record in result:\n",
    "                    src = record[\"source\"]\n",
    "                    rel = record[\"rel\"]\n",
    "                    tgt = record[\"target\"]\n",
    "                    triple_text = f\"{src} {rel} {tgt}.\"\n",
    "                    if triple_text not in seen_relations:\n",
    "                        seen_relations.add(triple_text)\n",
    "                        docs.append(Document(page_content=triple_text))\n",
    "\n",
    "        return docs\n",
    "\n",
    "    async def aget_relevant_documents(self, query: str) -> List[Document]:\n",
    "        # Pour la plupart des usages on peut renvoyer synchrone\n",
    "        return self.get_relevant_documents(query)\n",
    "\n",
    "    def __del__(self):\n",
    "        try:\n",
    "            self._driver.close()\n",
    "        except:\n",
    "            pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d343912d",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'load_dotenv' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mindexes\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mvectorstore\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m RetrievalQA\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# ----------------------------------------\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# 1. Charger les variables d’environnement\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# ----------------------------------------\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m \u001b[43mload_dotenv\u001b[49m()\n\u001b[32m      8\u001b[39m \u001b[38;5;66;03m# ----------------------------------------\u001b[39;00m\n\u001b[32m      9\u001b[39m \u001b[38;5;66;03m# 2. Instancier le LLM Albert\u001b[39;00m\n\u001b[32m     10\u001b[39m \u001b[38;5;66;03m# ----------------------------------------\u001b[39;00m\n\u001b[32m     11\u001b[39m llm = AlbertLLM(temperature=\u001b[32m0.2\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'load_dotenv' is not defined"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.indexes.vectorstore import RetrievalQA\n",
    "import dotenv\n",
    "\n",
    "# ----------------------------------------\n",
    "# 1. Charger les variables d’environnement\n",
    "# ----------------------------------------\n",
    "load_dotenv()\n",
    "\n",
    "# ----------------------------------------\n",
    "# 2. Instancier le LLM Albert\n",
    "# ----------------------------------------\n",
    "llm = AlbertLLM(temperature=0.2)\n",
    "\n",
    "# ----------------------------------------\n",
    "# 3. Créer le prompt template pour la QA\n",
    "# ----------------------------------------\n",
    "# {context} = textes du graphe Neo4j\n",
    "# {question} = question utilisateur\n",
    "prompt_template = \"\"\"\n",
    "Tu es un assistant expert en véhicules autonomes.\n",
    "Voici le contexte extrait d'un graphe de connaissances :\n",
    "{context}\n",
    "\n",
    "Question : {question}\n",
    "\n",
    "Réponds de façon précise, en t’appuyant seulement sur ces faits. Si ce n’est pas dans le contexte, répond « Désolé, je n’ai pas cette information. ».\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=prompt_template,\n",
    "    input_variables=[\"context\", \"question\"]\n",
    ")\n",
    "\n",
    "# ----------------------------------------\n",
    "# 4. Instancier le retriever Neo4j personnalisé\n",
    "# ----------------------------------------\n",
    "graph_retriever = Neo4jRetriever()\n",
    "\n",
    "# ----------------------------------------\n",
    "# 5. Construire la chaîne RetrievalQA\n",
    "# ----------------------------------------\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",           # on bourre tout le contexte d’un coup\n",
    "    retriever=graph_retriever,\n",
    "    return_source_documents=False,\n",
    "    chain_type_kwargs={\"prompt\": prompt}\n",
    ")\n",
    "\n",
    "# ----------------------------------------\n",
    "# 6. Boucle interactive\n",
    "# ----------------------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"=== Chat GraphRAG (Neo4j + Albert via LangChain) ===\")\n",
    "    while True:\n",
    "        question = input(\"\\nPose ta question (ou « exit » pour quitter) : \")\n",
    "        if question.lower().strip() in (\"exit\", \"quit\"):\n",
    "            break\n",
    "\n",
    "        # LangChain :\n",
    "        #  1) graph_retriever.get_relevant_documents(question) → liste de Docs\n",
    "        #  2) Concatène “context” = sommaire des docs, plus “question” dans le prompt\n",
    "        #  3) Envoie tout à llm._call(prompt_final) → Albert → génération\n",
    "        result = qa_chain({\"query\": question})\n",
    "        print(\"\\n📝 Réponse :\")\n",
    "        print(result[\"result\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
