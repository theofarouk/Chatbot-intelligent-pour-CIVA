{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a28fba69",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(\"outputs/all_triples.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    all_triples = json.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0ac3eef0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\maadt\\AppData\\Local\\Temp\\ipykernel_37420\\651748714.py:41: DeprecationWarning: write_transaction has been renamed to execute_write\n",
      "  session.write_transaction(insert_triplet, src, rel, tgt)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ 26746 triplets ins√©r√©s dans Neo4j.\n"
     ]
    }
   ],
   "source": [
    "from neo4j import GraphDatabase\n",
    "\n",
    "# ---------- 1. Configuration de la connexion Neo4j ----------\n",
    "# Remplacez ces valeurs par celles de votre instance Neo4j\n",
    "uri  = \"bolt://localhost:7687\"\n",
    "user = \"neo4j\"\n",
    "pwd  = \"BNEOsucks8921_\"\n",
    "\n",
    "# Cr√©e un driver Neo4j\n",
    "driver = GraphDatabase.driver(uri, auth=(user, pwd))\n",
    "\n",
    "\n",
    "# ---------- 2. Fonction d‚Äôinsertion d‚Äôun triplet dans Neo4j ----------\n",
    "def insert_triplet(tx, source, relation, target):\n",
    "    \"\"\"\n",
    "    Cr√©e ou r√©cup√®re deux n≈ìuds (Entity) nomm√©s 'source' et 'target', \n",
    "    puis cr√©e (si n√©cessaire) la relation entre eux.\n",
    "    Le type de relation est stock√© dans la propri√©t√© 'type' de l'arc.\n",
    "\n",
    "    Exemple Cypher g√©n√©r√© :\n",
    "      MERGE (a:Entity {name: $source})\n",
    "      MERGE (b:Entity {name: $target})\n",
    "      MERGE (a)-[:RELATION {type: $relation}]->(b)\n",
    "    \"\"\"\n",
    "    query = \"\"\"\n",
    "    MERGE (a:Entity {name: $source})\n",
    "    MERGE (b:Entity {name: $target})\n",
    "    MERGE (a)-[:RELATION {type: $relation}]->(b)\n",
    "    \"\"\"\n",
    "    tx.run(query, source=source, relation=relation, target=target)\n",
    "\n",
    "\n",
    "# ---------- 3. Fonction de chargement de tous les triplets ----------\n",
    "def load_all_triplets(triplets):\n",
    "    \"\"\"\n",
    "    Parcourt la liste 'triplets' (liste de tuples (source, relation, target))\n",
    "    et ex√©cute 'insert_triplet' pour chacun d‚Äôentre eux dans une m√™me session.\n",
    "    \"\"\"\n",
    "    with driver.session() as session:\n",
    "        for src, rel, tgt in triplets:\n",
    "            session.write_transaction(insert_triplet, src, rel, tgt)\n",
    "    print(f\"‚úÖ {len(triplets)} triplets ins√©r√©s dans Neo4j.\")\n",
    "\n",
    "\n",
    "# ---------- 4. Point d‚Äôentr√©e du script ----------\n",
    "if __name__ == \"__main__\":\n",
    "    # Supposons que vous ayez, dans un autre module, votre liste 'all_triples'\n",
    "    # Exemple : all_triples = [(\"Navette autonome\", \"test√©e dans\", \"Toulouse\"), ...]\n",
    "    # Il faut donc importer ou recr√©er cette liste ici.\n",
    "    #\n",
    "    # Si votre extraction a produit un module Python ou un fichier pickle,\n",
    "    # r√©cup√©rez la liste de tuples et assignez-la √† 'all_triples'.\n",
    "    #\n",
    "    # Par exemple, si vous avez sauvegard√© vos relations dans un fichier JSON :\n",
    "    #\n",
    "    # import json\n",
    "    # with open(\"outputs/all_triples.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    #     all_triples = json.load(f)\n",
    "    #\n",
    "    # Mais ici, illustrons un exemple statique (√† remplacer par votre propre liste) :\n",
    "\n",
    "    # Charge tous les triplets dans Neo4j\n",
    "    load_all_triplets(all_triples)\n",
    "\n",
    "    # Ferme le driver une fois termin√©\n",
    "    driver.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7d82ba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_all_triples():\n",
    "    \"\"\"\n",
    "    R√©cup√®re l'ensemble des triplets (entit√©, relation, entit√©) dans Neo4j.\n",
    "    \"\"\"\n",
    "    query = \"\"\"\n",
    "    MATCH (a:Source)-[r:RELATION]->(b:Target)\n",
    "    RETURN a.name AS source, r.type AS relation, b.name AS target\n",
    "    \"\"\"\n",
    "    triples = []\n",
    "    with driver.session() as session:\n",
    "        for record in session.run(query):\n",
    "            triples.append({\n",
    "                \"source\": record[\"source\"],\n",
    "                \"relation\": record[\"relation\"],\n",
    "                \"target\": record[\"target\"]\n",
    "            })\n",
    "    return triples\n",
    "\n",
    "# Exemple :\n",
    "all_triples = fetch_all_triples()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "93db417a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.service_context_elements.llm_predictor import LLMPredictor\n",
    "from llama_index.core import PromptHelper, ServiceContext\n",
    "from llama_index.cli.rag.base import LLM\n",
    "from llama_index.core.indices import KnowledgeGraphIndex\n",
    "#from llama_index.indices.knowledge_graph.schema import KGTable\n",
    "from llama_index.graph_stores.neo4j import Neo4jPropertyGraphStore\n",
    "from dotenv import load_dotenv\n",
    "from llama_index.core.indices import PropertyGraphIndex\n",
    "import os\n",
    "import requests\n",
    "from neo4j import GraphDatabase\n",
    "import openai\n",
    "from langchain.llms.base import LLM\n",
    "from typing import Optional, List, Mapping, Any\n",
    "from pydantic import BaseModel\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "25cdc73b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) On configure openai pour pointer vers Albert\n",
    "openai.api_key = 'sk-eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJ1c2VyX2lkIjo4NDAyLCJ0b2tlbl9pZCI6MTQ4OSwiZXhwaXJlc19hdCI6MTc4MDM1MTIwMH0.mOB9Cx4U4G7K5gin0twePc_WauAEPtRWQ0UaK6oUs9I'\n",
    "openai.api_base = \"https://albert.api.etalab.gouv.fr/v1\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a3fc45e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AlbertLLM(LLM, BaseModel):\n",
    "    \"\"\"\n",
    "    Wrapper LangChain pour Albert (API OpenAI-compatible).\n",
    "    \"\"\"\n",
    "\n",
    "    temperature: float = 0.2\n",
    "    model_name: str = \"albert-small\"\n",
    "\n",
    "    class Config:\n",
    "        \"\"\"Pour que pydantic accepte les champs suppl√©mentaires (ignorez-les).\"\"\"\n",
    "        extra = \"ignore\"\n",
    "\n",
    "    def _call(self, prompt: str, stop: Optional[List[str]] = None) -> str:\n",
    "        \"\"\"\n",
    "        Envoie le prompt √† l‚ÄôAPI Albert et renvoie le texte g√©n√©r√©.\n",
    "        \"\"\"\n",
    "        response = openai.ChatCompletion.create(\n",
    "            model=self.model_name,\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            temperature=self.temperature,\n",
    "            stop=stop,\n",
    "            max_tokens=1024,\n",
    "        )\n",
    "        return response.choices[0].message.content\n",
    "\n",
    "    @property\n",
    "    def _identifying_params(self) -> Mapping[str, Any]:\n",
    "        return {\"model_name\": self.model_name, \"temperature\": self.temperature}\n",
    "\n",
    "    @property\n",
    "    def _llm_type(self) -> str:\n",
    "        return \"albert\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a51495e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\maadt\\AppData\\Local\\Temp\\ipykernel_39392\\3056014003.py:7: DeprecationWarning: Retrievers must implement abstract `_get_relevant_documents` method instead of `get_relevant_documents`\n",
      "  class Neo4jRetriever(BaseRetriever):\n",
      "C:\\Users\\maadt\\AppData\\Local\\Temp\\ipykernel_39392\\3056014003.py:7: DeprecationWarning: Retrievers must implement abstract `_aget_relevant_documents` method instead of `aget_relevant_documents`\n",
      "  class Neo4jRetriever(BaseRetriever):\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "import os\n",
    "from typing import List, Any\n",
    "from neo4j import GraphDatabase\n",
    "from langchain.schema import BaseRetriever, Document\n",
    "from pydantic import PrivateAttr\n",
    "\n",
    "\n",
    "class Neo4jRetriever(BaseRetriever):\n",
    "    \"\"\"\n",
    "    Retriever LangChain qui interroge Neo4j pour ramener un sous-graphe pertinent.\n",
    "    \"\"\"\n",
    "\n",
    "    # D√©clare driver comme attribut priv√© afin que Pydantic ne l'exige pas comme champ\n",
    "    _driver: Any = PrivateAttr()\n",
    "\n",
    "    def __init__(self):\n",
    "        # Appelle le constructeur de BaseModel\n",
    "        super().__init__()\n",
    "\n",
    "        uri = os.getenv(\"NEO4J_URI\")\n",
    "        user = os.getenv(\"NEO4J_USER\")\n",
    "        pwd  = os.getenv(\"NEO4J_PWD\")\n",
    "        self._driver = GraphDatabase.driver(uri, auth=(user, pwd))\n",
    "\n",
    "    def get_relevant_documents(self, query: str) -> List[Document]:\n",
    "        \"\"\"\n",
    "        1) On extrait des tokens (mots) de la question,\n",
    "        2) On interroge Neo4j pour chaque token correspondant\n",
    "           √† un n≈ìud Entity.name,\n",
    "        3) On construit un Document par relation trouv√©e.\n",
    "        \"\"\"\n",
    "        # Tokenisation basique ; dans la vraie vie, on ferait un NER ou des lowercase+strip\n",
    "        tokens = [tok.strip() for tok in query.split() if len(tok) > 1]\n",
    "        seen_relations = set()\n",
    "        docs: List[Document] = []\n",
    "\n",
    "        with self._driver.session() as session:\n",
    "            for tok in tokens:\n",
    "                cypher = \"\"\"\n",
    "                MATCH (n:Entity {name: $name})-[r]-(m:Entity)\n",
    "                RETURN n.name AS source, type(r) AS rel, m.name AS target\n",
    "                \"\"\"\n",
    "                result = session.run(cypher, name=tok)\n",
    "                for record in result:\n",
    "                    src = record[\"source\"]\n",
    "                    rel = record[\"rel\"]\n",
    "                    tgt = record[\"target\"]\n",
    "                    triple_text = f\"{src} {rel} {tgt}.\"\n",
    "                    if triple_text not in seen_relations:\n",
    "                        seen_relations.add(triple_text)\n",
    "                        docs.append(Document(page_content=triple_text))\n",
    "\n",
    "        return docs\n",
    "\n",
    "    async def aget_relevant_documents(self, query: str) -> List[Document]:\n",
    "        # Pour la plupart des usages on peut renvoyer synchrone\n",
    "        return self.get_relevant_documents(query)\n",
    "\n",
    "    def __del__(self):\n",
    "        try:\n",
    "            self._driver.close()\n",
    "        except:\n",
    "            pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d343912d",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'load_dotenv' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mindexes\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mvectorstore\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m RetrievalQA\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# ----------------------------------------\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# 1. Charger les variables d‚Äôenvironnement\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# ----------------------------------------\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m \u001b[43mload_dotenv\u001b[49m()\n\u001b[32m      8\u001b[39m \u001b[38;5;66;03m# ----------------------------------------\u001b[39;00m\n\u001b[32m      9\u001b[39m \u001b[38;5;66;03m# 2. Instancier le LLM Albert\u001b[39;00m\n\u001b[32m     10\u001b[39m \u001b[38;5;66;03m# ----------------------------------------\u001b[39;00m\n\u001b[32m     11\u001b[39m llm = AlbertLLM(temperature=\u001b[32m0.2\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'load_dotenv' is not defined"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.indexes.vectorstore import RetrievalQA\n",
    "import dotenv\n",
    "\n",
    "# ----------------------------------------\n",
    "# 1. Charger les variables d‚Äôenvironnement\n",
    "# ----------------------------------------\n",
    "load_dotenv()\n",
    "\n",
    "# ----------------------------------------\n",
    "# 2. Instancier le LLM Albert\n",
    "# ----------------------------------------\n",
    "llm = AlbertLLM(temperature=0.2)\n",
    "\n",
    "# ----------------------------------------\n",
    "# 3. Cr√©er le prompt template pour la QA\n",
    "# ----------------------------------------\n",
    "# {context} = textes du graphe Neo4j\n",
    "# {question} = question utilisateur\n",
    "prompt_template = \"\"\"\n",
    "Tu es un assistant expert en v√©hicules autonomes.\n",
    "Voici le contexte extrait d'un graphe de connaissances :\n",
    "{context}\n",
    "\n",
    "Question : {question}\n",
    "\n",
    "R√©ponds de fa√ßon pr√©cise, en t‚Äôappuyant seulement sur ces faits. Si ce n‚Äôest pas dans le contexte, r√©pond ¬´ D√©sol√©, je n‚Äôai pas cette information. ¬ª.\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=prompt_template,\n",
    "    input_variables=[\"context\", \"question\"]\n",
    ")\n",
    "\n",
    "# ----------------------------------------\n",
    "# 4. Instancier le retriever Neo4j personnalis√©\n",
    "# ----------------------------------------\n",
    "graph_retriever = Neo4jRetriever()\n",
    "\n",
    "# ----------------------------------------\n",
    "# 5. Construire la cha√Æne RetrievalQA\n",
    "# ----------------------------------------\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",           # on bourre tout le contexte d‚Äôun coup\n",
    "    retriever=graph_retriever,\n",
    "    return_source_documents=False,\n",
    "    chain_type_kwargs={\"prompt\": prompt}\n",
    ")\n",
    "\n",
    "# ----------------------------------------\n",
    "# 6. Boucle interactive\n",
    "# ----------------------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"=== Chat GraphRAG (Neo4j + Albert via LangChain) ===\")\n",
    "    while True:\n",
    "        question = input(\"\\nPose ta question (ou ¬´ exit ¬ª pour quitter) : \")\n",
    "        if question.lower().strip() in (\"exit\", \"quit\"):\n",
    "            break\n",
    "\n",
    "        # LangChain :\n",
    "        #  1) graph_retriever.get_relevant_documents(question) ‚Üí liste de Docs\n",
    "        #  2) Concat√®ne ‚Äúcontext‚Äù = sommaire des docs, plus ‚Äúquestion‚Äù dans le prompt\n",
    "        #  3) Envoie tout √† llm._call(prompt_final) ‚Üí Albert ‚Üí g√©n√©ration\n",
    "        result = qa_chain({\"query\": question})\n",
    "        print(\"\\nüìù R√©ponse :\")\n",
    "        print(result[\"result\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
